{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyO3K2zO9v6wwwFqVCGs38KW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paudelsushil/labelcombinations/blob/main/project_adleo_geog315_spring24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Objective3:\n",
        "# DeepLab3+ Model\n",
        "DeepLabv3+ utilizes an encoder-decoder structure to perform image segmentation. The encoder extracts shallow and high-level semantic information from the image, while the decoder combines low-level and high-level features to improve the accuracy of segmentation boundaries and classify the semantic information of different pixels [Chen et al., (2018)](https://link.springer.com/content/pdf/10.1007/978-3-030-01234-2_49.pdf?pdf=inline%20link).\n",
        "\n",
        "This project is based on the improved classis DeepLabv3+ network model proposed by [Chen et al.,(2023)](https://link.springer.com/content/pdf/10.1007/s40747-023-01304-z.pdf).\n",
        "\n",
        "## Architecture of improved DeepLabv3+ with MobileNetv2 backbone\n",
        "\n",
        "**`A. Encoder`**\n",
        " 1. `Backbone` : lightweight network `MobileNetv2` in place of Xception.\n",
        " 2. `ASPP` : `Hybrid Dialted Convolution` (HDC) module to alleviate the gridding effect. In addition,  `Strip Pooling Module` is used instead of spatial mean pooling to improve th elocal segmentation effect.\n",
        " 3. `Normalization-based Attention Module` (NAM): This lightweight attention mechanism is also applied to the stacked compressed high-level feature maps to help improve the segmentation accuracy of the image.\n",
        "\n",
        "**`B. Decoder`**\n",
        "1. `NAM`: The seventh layer feature with `NAM` attention is upsampled to the same size as the fourth layer feature after fusion and channel adjustment.\n",
        "2. `ResNet50`: This module is added to obtain riccher low-level target feature information.\n",
        "3. `Concatenate`: The **deep features** and **shallow features** are concatenated as in the original model.\n",
        "4. `Upsampling`: After a 3 X 3 convolution and 4 X `upsampling`, the image is restored to its original size.\n",
        "\n",
        "\n",
        " [Architecture Image]()"
      ],
      "metadata": {
        "id": "Av8BX6v5mL_s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data Preparation"
      ],
      "metadata": {
        "id": "-byRvwK76QOk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehcJSZG5ad6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cad05efc-ae85-4a22-d8b0-ba5524a87dae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install required packages\n",
        "\n",
        "%%capture\n",
        "!pip install rasterio\n"
      ],
      "metadata": {
        "id": "TB_rgHf166Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required packages\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "import tqdm # Adds a smart progress meter to any iterable or file operation\n",
        "\n",
        "import math\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import cv2\n",
        "import rasterio\n",
        "#  defines a rectangular area within the raster using four properties\n",
        "# xoff, yoff, width, height\n",
        "from rasterio.windows import Window\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import logging\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "import itertools\n",
        "\n",
        "\n",
        "from IPython.core.debugger import set_trace # Insert a breakpoint into the code\n",
        "from IPython.display import Image\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "XNc4dQUl7RAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions"
      ],
      "metadata": {
        "id": "4FzjUpiAg00A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InputError(Exception):\n",
        "    '''\n",
        "    Exception raised for errors in the input\n",
        "    '''\n",
        "    def __init__(self, message):\n",
        "        '''\n",
        "        Params:\n",
        "            message (str): explanation of the error\n",
        "        '''\n",
        "        self.message = message\n",
        "    def __str__(self):\n",
        "        '''\n",
        "        Define message to return when error is raised\n",
        "        '''\n",
        "        if self.message:\n",
        "            return 'InputError, {} '.format(self.message)\n",
        "        else:\n",
        "            return 'InputError'\n",
        "# =============================================================================\n",
        "def load_data(data_path, usage=\"train\", window=None, norm_stats_type=None,\n",
        "              is_label=False):\n",
        "    '''\n",
        "    Read geographic data into numpy array\n",
        "    Params:\n",
        "        data_path : str\n",
        "            Path of data to load\n",
        "        usage : str\n",
        "            Usage of the data: \"train\", \"validate\", or \"predict\"\n",
        "        window : tuple\n",
        "            The view onto a rectangular subset of the data, in the format of\n",
        "            (column offsets, row offsets, width in pixel, height in pixel)\n",
        "        norm_stats_type : str\n",
        "            How the normalization statistics is calculated.\n",
        "        is_label : binary\n",
        "            Decide whether to saturate data with tested threshold\n",
        "    Returns:\n",
        "        narray\n",
        "    '''\n",
        "    # Open the data file using the 'rasterio' library\n",
        "    with rasterio.open(data_path, \"r\") as src:\n",
        "      # Check if the data is a label (segmentation mask)\n",
        "        if is_label:\n",
        "            if src.count != 1:  # Ensure the label has a single channel\n",
        "                raise InputError(\"Label shape not applicable: \\\n",
        "                                expected 1 channel\")\n",
        "            img = src.read(1)  # Read the single channel of the label data\n",
        "\n",
        "        else:\n",
        "        # Store the value representing 'no data' in the image\n",
        "            nodata = src.nodata\n",
        "            # Verify normalization type is valid\n",
        "            assert norm_stats_type in [\"local_per_tile\", \"local_per_band\",\n",
        "                                      \"global_per_band\"]\n",
        "\n",
        "            if norm_stats_type == \"local_per_tile\":\n",
        "              # Apply per-tile normalization\n",
        "                img = mmnorm1(src.read(), nodata=nodata)\n",
        "            elif norm_stats_type == \"local_per_band\":\n",
        "              # Per-band normalization, clipping values\n",
        "                img = mmnorm2(src.read(), nodata=nodata, clip_val=1.5)\n",
        "            elif norm_stats_type == \"global_per_band\":\n",
        "              # Global per-band normalization, clipping values\n",
        "                img = mmnorm3(src.read(), nodata=nodata, clip_val=1.5)\n",
        "\n",
        "            # For 'train' or 'validate' subsets\n",
        "            if usage in ['train', 'validate']:\n",
        "              # Extract a specific window from the image\n",
        "                img = img[:, max(0, window[1]): window[1] + window[3],\n",
        "                          max(0, window[0]): window[0] + window[2]]\n",
        "\n",
        "    return img  # Return the processed image or label data\n",
        "# ==============================================================================\n",
        "\n",
        "def get_stacked_img(img_paths, usage, norm_stats_type=\"local_per_tile\",\n",
        "                    window=None):\n",
        "    '''\n",
        "    Read geographic data into numpy array\n",
        "    Params:\n",
        "        gsPath :str\n",
        "            Path of growing season image\n",
        "        osPath : str\n",
        "            Path of off season image\n",
        "        img_paths : list\n",
        "            List of paths for imgages\n",
        "        usage : str\n",
        "            Usage of the image: \"train\", \"validate\", or \"predict\"\n",
        "        norm_stats_type : str\n",
        "            How the normalization statistics is calculated.\n",
        "        window : tuple\n",
        "            The view onto a rectangular subset of the data, in the\n",
        "            format of (column offsets, row offsets, width in pixel, height in\n",
        "            pixel)\n",
        "    Returns:\n",
        "        ndarray\n",
        "    '''\n",
        "\n",
        "    if len(img_paths) > 1:  # If there are multiple image paths:\n",
        "      img_ls = [load_data(m, usage, window, norm_stats_type) for m in img_paths]\n",
        "      # Load data for each image path, potentially applying normalization\n",
        "      img = np.concatenate(img_ls, axis=0).transpose(1, 2, 0)\n",
        "      # Combine the loaded data into a single array and rearrange dimensions\n",
        "    else:  # If there's only a single image path:\n",
        "      # Load data for the single image path and rearrange dimensions\n",
        "      img = load_data(img_paths[0], usage, \\\n",
        "                      window, norm_stats_type).transpose(1, 2, 0)\n",
        "\n",
        "    # For 'train' or 'validate' subsets:\n",
        "    if usage in [\"train\", \"validate\"]:\n",
        "      # Extract window parameters\n",
        "      col_off, row_off, col_target, row_target = window\n",
        "      row, col, c = img.shape  # Get image dimensions\n",
        "\n",
        "      # Check if image is smaller than the target window\n",
        "      if row < row_target or col < col_target:\n",
        "          row_off = abs(row_off) if row_off < 0 else 0  # Adjust offsets\n",
        "          col_off = abs(col_off) if col_off < 0 else 0\n",
        "\n",
        "          # Create a larger blank canvas\n",
        "          canvas = np.zeros((row_target, col_target, c))\n",
        "          # Place image onto canvas\n",
        "          canvas[row_off: row_off + row, col_off : col_off + col, :] = img\n",
        "          return canvas  # Return the canvas with the padded image\n",
        "\n",
        "      else:\n",
        "          return img  # The image fits the window, so return it directly\n",
        "\n",
        "    elif usage == \"predict\":  # For prediction purposes:\n",
        "      return img  # Return the image as is\n",
        "\n",
        "    else:\n",
        "      raise ValueError  # Invalid 'usage' value\n",
        "\n",
        "# ==============================================================================\n",
        "def get_buffered_window(src_path, dst_path, buffer):\n",
        "    '''\n",
        "    Get bounding box representing subset of source image that overlaps with\n",
        "    bufferred destination image, in format of (column offsets, row offsets,\n",
        "    width, height)\n",
        "\n",
        "    Params:\n",
        "        src_path : str\n",
        "            Path of source image to get subset bounding box\n",
        "        dst_path : str\n",
        "            Path of destination image as a reference to define the\n",
        "            bounding box. Size of the bounding box is\n",
        "            (destination width + buffer * 2, destination height + buffer * 2)\n",
        "        buffer :int\n",
        "            Buffer distance of bounding box edges to destination image\n",
        "            measured by pixel numbers\n",
        "\n",
        "    Returns:\n",
        "        tuple in form of (column offsets, row offsets, width, height)\n",
        "    '''\n",
        "\n",
        "    with rasterio.open(src_path, \"r\") as src:\n",
        "        gt_src = src.transform\n",
        "\n",
        "    with rasterio.open(dst_path, \"r\") as dst:\n",
        "        gt_dst = dst.transform\n",
        "        w_dst = dst.width\n",
        "        h_dst = dst.height\n",
        "\n",
        "    col_off = round((gt_dst[2] - gt_src[2]) / gt_src[0]) - buffer\n",
        "    row_off = round((gt_dst[5] - gt_src[5]) / gt_src[4]) - buffer\n",
        "    width = w_dst + buffer * 2\n",
        "    height = h_dst + buffer * 2\n",
        "\n",
        "    return col_off, row_off, width, height\n",
        "\n",
        "# ==============================================================================\n",
        "\n",
        "def get_meta_from_bounds(file, buffer):\n",
        "    '''\n",
        "    Get metadata of unbuffered region in given file\n",
        "    Params:\n",
        "        file (str):  File name of a image chip\n",
        "        buffer (int): Buffer distance measured by pixel numbers\n",
        "    Returns:\n",
        "        dictionary\n",
        "    '''\n",
        "\n",
        "    with rasterio.open(file, \"r\") as src:\n",
        "\n",
        "        meta = src.meta\n",
        "        dst_width = src.width - 2 * buffer\n",
        "        dst_height = src.height - 2 * buffer\n",
        "\n",
        "        window = Window(buffer, buffer, dst_width, dst_height)\n",
        "        win_transform = src.window_transform(window)\n",
        "\n",
        "    meta.update({\n",
        "        'width': dst_width,\n",
        "        'height': dst_height,\n",
        "        'transform': win_transform,\n",
        "        'count': 1,\n",
        "        'nodata': -128,\n",
        "        'dtype': 'int8'\n",
        "    })\n",
        "\n",
        "    return meta\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "def display_hist(img):\n",
        "    '''\n",
        "    Display data distribution of input image in a histogram\n",
        "    Params:\n",
        "        img (narray): Image in form of (H,W,C) to display data distribution\n",
        "    '''\n",
        "\n",
        "    img = mmnorm1(img)\n",
        "    im = np.where(img == 0, np.nan, img)\n",
        "\n",
        "    plt.hist(img.ravel(), 500, [np.nanmin(im), img.max()])\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    plt.show()\n",
        "\n",
        "# ==============================================================================\n",
        "def mmnorm1(img, nodata):\n",
        "    '''\n",
        "    Data normalization with min/max method\n",
        "    Params:\n",
        "        img (narray): The targeted image for normalization\n",
        "    Returns:\n",
        "        narrray\n",
        "    '''\n",
        "\n",
        "    img_tmp = np.where(img == nodata, np.nan, img)\n",
        "    img_max = np.nanmax(img_tmp)\n",
        "    img_min = np.nanmin(img_tmp)\n",
        "    normalized = (img - img_min) / (img_max - img_min)\n",
        "    normalized = np.clip(normalized, 0, 1)\n",
        "\n",
        "    return normalized\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "def mmnorm2(img, nodata, clip_val=None):\n",
        "    r\"\"\"\n",
        "    Normalize the input image pixels to [0, 1] ranged based on the\n",
        "    minimum and maximum statistics of each band per tile.\n",
        "    Arguments:\n",
        "            img : numpy array\n",
        "                Stacked image bands with a dimension of (C,H,W).\n",
        "            nodata : str\n",
        "                Value reserved to represent NoData in the image chip.\n",
        "            clip_val : int\n",
        "                Defines how much of the distribution tails to be cut off.\n",
        "    Returns:\n",
        "            img : numpy array\n",
        "                Normalized image stack of size (C,H,W).\n",
        "    Note 1: If clip then min, max are calculated from the clipped image.\n",
        "    \"\"\"\n",
        "\n",
        "    # filter out zero pixels in generating statistics.\n",
        "    nan_corr_img = np.where(img == nodata, np.nan, img)\n",
        "    nan_corr_img = np.where(img == 0, np.nan, img)\n",
        "\n",
        "    if clip_val > 0:\n",
        "        left_tail_clip = np.nanpercentile(nan_corr_img, clip_val)\n",
        "        right_tail_clip = np.nanpercentile(nan_corr_img, 100 - clip_val)\n",
        "\n",
        "        left_clipped_img = np.where(img < left_tail_clip, left_tail_clip, img)\n",
        "        clipped_img = np.where(left_clipped_img > right_tail_clip,\n",
        "                               right_tail_clip, left_clipped_img)\n",
        "\n",
        "        normalized_bands = []\n",
        "        for i in range(img.shape[0]):\n",
        "            band_min = np.nanmin(clipped_img[i, :, :])\n",
        "            band_max = np.nanmax(clipped_img[i, :, :])\n",
        "            normalized_band = (clipped_img[i, :, :] - band_min) /\\\n",
        "                (band_max - band_min)\n",
        "            normalized_bands.append(np.expand_dims(normalized_band, 0))\n",
        "        normal_img = np.concatenate(normalized_bands, 0)\n",
        "\n",
        "    elif clip_val == 0 or clip_val is None:\n",
        "        normalized_bands = []\n",
        "        for i in range(img.shape[0]):\n",
        "            band_min = np.nanmin(nan_corr_img[i, :, :])\n",
        "            band_max = np.nanmax(nan_corr_img[i, :, :])\n",
        "            normalized_band = (nan_corr_img[i, :, :] - band_min) /\\\n",
        "                (band_max - band_min)\n",
        "            normalized_bands.append(np.expand_dims(normalized_band, 0))\n",
        "        normal_img = np.concatenate(normalized_bands, 0)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"clip must be a non-negative decimal.\")\n",
        "\n",
        "    normal_img = np.clip(normal_img, 0, 1)\n",
        "    return normal_img\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "def mmnorm3(img, nodata, clip_val=None):\n",
        "    hardcoded_stats = {\n",
        "        \"mins\": np.array([331.0, 581.0, 560.0, 1696.0]),\n",
        "        \"maxs\": np.array([1403.0, 1638.0, 2076.0, 3652.0])\n",
        "    }\n",
        "\n",
        "    num_bands = img.shape[0]\n",
        "    mins = hardcoded_stats[\"mins\"]\n",
        "    maxs = hardcoded_stats[\"maxs\"]\n",
        "\n",
        "    if clip_val:\n",
        "        normalized_bands = []\n",
        "        for i in range(num_bands):\n",
        "            nan_corr_img = np.where(img[i, :, :] == nodata, np.nan,\n",
        "                                    img[i, :, :])\n",
        "            nan_corr_img = np.where(img[i, :, :] == 0, np.nan, img[i, :, :])\n",
        "            left_tail_clip = np.nanpercentile(nan_corr_img, clip_val)\n",
        "            right_tail_clip = np.nanpercentile(nan_corr_img, 100 - clip_val)\n",
        "            left_clipped_band = np.where(img[i, :, :] < left_tail_clip,\n",
        "                                         left_tail_clip, img[i, :, :])\n",
        "            clipped_band = np.where(left_clipped_band > right_tail_clip,\n",
        "                                    right_tail_clip, left_clipped_band)\n",
        "            normalized_band = (clipped_band - mins[i]) / (maxs[i] - mins[i])\n",
        "            normalized_bands.append(np.expand_dims(normalized_band, 0))\n",
        "        img = np.concatenate(normalized_bands, 0)\n",
        "\n",
        "    else:\n",
        "        for i in range(num_bands):\n",
        "            img[i, :, :] = (img[i, :, :] - mins[i]) / (maxs[i] - mins[i])\n",
        "\n",
        "    img = np.clip(img, 0, 1)\n",
        "    return img\n",
        "\n",
        "# ==============================================================================\n",
        "def get_chips(img, dsize, buffer):\n",
        "    '''\n",
        "    Generate small chips from input images and the corresponding index of each\n",
        "    chip The index marks the location of corresponding upper-left pixel of a\n",
        "    chip.\n",
        "    Params:\n",
        "        img (narray): Image in format of (H,W,C) to be crop, in this case it is\n",
        "            the concatenated image of growing season and off season\n",
        "        dsize (int): Cropped chip size\n",
        "        buffer (int):Number of overlapping pixels when extracting images chips\n",
        "    Returns:\n",
        "        list of cropped chips and corresponding coordinates\n",
        "    '''\n",
        "\n",
        "    h, w, _ = img.shape\n",
        "    x_ls = range(0,h - 2 * buffer, dsize - 2 * buffer)\n",
        "    y_ls = range(0, w - 2 * buffer, dsize - 2 * buffer)\n",
        "\n",
        "    index = list(itertools.product(x_ls, y_ls))\n",
        "\n",
        "    img_ls = []\n",
        "    for i in range(len(index)):\n",
        "        x, y = index[i]\n",
        "        img_ls.append(img[x:x + dsize, y:y + dsize, :])\n",
        "\n",
        "    return img_ls, index\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "def display(img, label, mask):\n",
        "\n",
        "    '''\n",
        "    Display composites and their labels\n",
        "    Params:\n",
        "        img (torch.tensor): Image in format of (C,H,W)\n",
        "        label (torch.tensor): Label in format of (H,W)\n",
        "        mask (torch.tensor): Mask in format of (H,W)\n",
        "    '''\n",
        "\n",
        "    gsimg = (comp432_dis(img, \"GS\") * 255).permute(1, 2, 0).int()\n",
        "    osimg = (comp432_dis(img, \"OS\") * 255).permute(1, 2, 0).int()\n",
        "\n",
        "\n",
        "    _, figs = plt.subplots(1, 4, figsize=(20, 20))\n",
        "\n",
        "    label = label.cpu()\n",
        "\n",
        "    figs[0].imshow(gsimg)\n",
        "    figs[1].imshow(osimg)\n",
        "    figs[2].imshow(label)\n",
        "    figs[3].imshow(mask)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# ==============================================================================\n",
        "# color composite\n",
        "def comp432_dis(img, season):\n",
        "    '''\n",
        "    Generate false color composites\n",
        "    Params:\n",
        "        img (torch.tensor): Image in format of (C,H,W)\n",
        "        season (str): Season of the composite to generate, be  \"GS\" or \"OS\"\n",
        "    '''\n",
        "\n",
        "    viewsize = img.shape[1:]\n",
        "\n",
        "    if season == \"GS\":\n",
        "\n",
        "        b4 = mmnorm1(img[3, :, :].cpu().view(1, *viewsize),0)\n",
        "        b3 = mmnorm1(img[2, :, :].cpu().view(1, *viewsize),0)\n",
        "        b2 = mmnorm1(img[1, :, :].cpu().view(1, *viewsize),0)\n",
        "\n",
        "    elif season == \"OS\":\n",
        "        b4 = mmnorm1(img[7, :, :].cpu().view(1, *viewsize), 0)\n",
        "        b3 = mmnorm1(img[6, :, :].cpu().view(1, *viewsize), 0)\n",
        "        b2 = mmnorm1(img[5, :, :].cpu().view(1, *viewsize), 0)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Bad season value\")\n",
        "\n",
        "    img = torch.cat([b4, b3, b2], 0)\n",
        "\n",
        "    return img\n",
        "\n",
        "# ==============================================================================\n",
        "def make_reproducible(seed=42, cudnn=True):\n",
        "    \"\"\"Make all the randomization processes start from a shared seed\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.random.manual_seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    if cudnn:\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "# ==============================================================================\n",
        "\n",
        "def pickle_dataset(dataset, file_path):\n",
        "    with open(file_path, \"wb\") as fp:\n",
        "              pickle.dump(dataset, fp)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "def load_pickle(file_path):\n",
        "    return pd.read_pickle(file_path)\n",
        "\n",
        "# ==============================================================================\n",
        "def progress_reporter(msg, verbose, logger=None):\n",
        "    \"\"\"Helps control print statements and log writes\n",
        "    Parameters\n",
        "    ----------\n",
        "    msg : str\n",
        "      Message to write out\n",
        "    verbose : bool\n",
        "      Prints or not to console\n",
        "    logger : logging.logger\n",
        "      logger (defaults to none)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "        Message to console and or log\n",
        "    \"\"\"\n",
        "\n",
        "    if verbose:\n",
        "        print(msg)\n",
        "\n",
        "    if logger:\n",
        "        logger.info(msg)\n",
        "\n",
        "# ==============================================================================\n",
        "def setup_logger(log_dir, log_name, use_date=False):\n",
        "    \"\"\"Create logger\n",
        "    \"\"\"\n",
        "    if use_date:\n",
        "        dt = datetime.now().strftime(\"%d%m%Y_%H%M\")\n",
        "        log = \"{}/{}_{}.log\".format(log_dir, log_name, dt)\n",
        "    else:\n",
        "        log = \"{}/{}.log\".format(log_dir, log_name)\n",
        "\n",
        "    for handler in logging.root.handlers[:]:\n",
        "        logging.root.removeHandler(handler)\n",
        "    log_format = (\n",
        "        f\"%(asctime)s::%(levelname)s::%(name)s::%(filename)s::\"\n",
        "        f\"%(lineno)d::%(message)s\"\n",
        "    )\n",
        "    logging.basicConfig(filename=log, filemode='w',\n",
        "                        level=logging.INFO, format=log_format)\n",
        "\n",
        "    return logging.getLogger()"
      ],
      "metadata": {
        "id": "7aXBJ3ALg0bW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation\n",
        "1. Find the suitable `image dataset` to apply `improved DeepLabv3+ model` for the image segmentation process.\n",
        "  - For this task, we used image dataset that was used in `S. Khallaghi, (2024) ch. 2`.\n",
        "2. Prepare the `labels (pixel-wise annotations)` that are compatible with selected image dataset.\n",
        "  - For this task, we filtered [all_class_cataloge](/content/gdrive/MyDrive/adleo/project_data/label_catalog_allclasses.csv) using methods and functions given in [notebook](https://github.com/paudelsushil/labelcombinations/blob/main/Make_Labels_ADLEO_Final.ipynb) and prepared final [filtered cataloge](/content/gdrive/MyDrive/adleo/project_data/label-catalog-filtered.csv) to get our pixel-wise annotations as a [lable images](/content/gdrive/MyDrive/adleo/project_data/labels).\n"
      ],
      "metadata": {
        "id": "pzxo77ik_UJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tirz3g1JIeAr",
        "outputId": "bb359441-e102-45c0-a611-7b9e27d92c67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Dataset for training, validating, and testing the model"
      ],
      "metadata": {
        "id": "rAQQOPVvBqAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the datasets source and workingFolder source\n",
        "src_dir = \"/content/gdrive/MyDrive/adleo/project_data\"\n",
        "\n",
        "WorkingFolder = \"/content/gdrive/MyDrive/adleo/project_data\"\n",
        "\n",
        "\n",
        "# Define the path for images, lables, and cataloge\n",
        "# Image path\n",
        "img_paths = list(Path(os.path.join(src_dir, \"images\")).glob(\"*.tif\"))\n",
        "\n",
        "# Label path\n",
        "lbl_paths = list(Path(os.path.join(src_dir, \"labels\")).glob(\"*.tif\"))\n",
        "\n",
        "# Label cataloge\n",
        "cataloge = pd.read_csv(os.path.join(src_dir, \"label-catalog-filtered.csv\"))\n",
        "\n",
        "# Check if all paths are valid lists\n",
        "if not all(isinstance(path_list, list) for path_list in (img_paths, lbl_paths)):\n",
        "    raise ValueError(\"Both image_paths and label_paths must be lists.\")\n",
        "\n",
        "# Prints valid number of images and labels\n",
        "print(\"No. of images:\",len(img_paths), \"\\n\",\n",
        "      \"No. of labels:\", len(lbl_paths),\"\\n\",\n",
        "      \"No. of rows in cataloge:\", len(cataloge))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MA9ZtQWP93z5",
        "outputId": "9fb184c0-2660-4efb-fb5a-f1cc213ec48b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of images: 33873 \n",
            " No. of labels: 33756 \n",
            " No. of rows in cataloge: 33746\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-71-f3a55325f6a4>:15: DtypeWarning: Columns (2,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  cataloge = pd.read_csv(os.path.join(src_dir, \"label-catalog-filtered.csv\"))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-process Datasets:\n",
        "1. `Resize images` to a standard size suitable for model.\n",
        "\n",
        "2. `Normalize pixel values`(e.g., scale to range 0-1 or subtract mean).\n",
        "\n",
        "3. `Image Augmentation`  (e.g., random flipping, cropping).\n"
      ],
      "metadata": {
        "id": "xBHQIHCUCBrY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SjT_uj9kGGVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image Normalization"
      ],
      "metadata": {
        "id": "upnS_NL-Cfoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def min_max_normalize_image(image, dtype=np.float32):\n",
        "    \"\"\"\n",
        "    image_path(str) : Absolute path to the image patch.\n",
        "    dtype (numpy datatype) : data type of the normalized image default is\n",
        "    \"np.float32\".\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the minimum and maximum values for each band\n",
        "    min_values = np.nanmin(image, axis=(1, 2))[:, np.newaxis, np.newaxis]\n",
        "    max_values = np.nanmax(image, axis=(1, 2))[:, np.newaxis, np.newaxis]\n",
        "\n",
        "    # Normalize the image data to the range [0, 1]\n",
        "    normalized_img = (image - min_values) / (max_values - min_values)\n",
        "\n",
        "    # Return the normalized image data\n",
        "    return normalized_img"
      ],
      "metadata": {
        "id": "cGI_pLF0Ce6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image Augmentation"
      ],
      "metadata": {
        "id": "tj_af9t_Cr_p"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lFJyV0MyCMe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Active dataset loading pipeline\n"
      ],
      "metadata": {
        "id": "nYZ6dLjEqZvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AquacultureData(Dataset):\n",
        "    def __init__(self, src_dir, usage, dataset_name=None,\n",
        "                 apply_normalization=False, transform=None, csv_name=None,\n",
        "                 patch_size=None, overlap=None, catalog_index=None):\n",
        "        r\"\"\"\n",
        "        src_dir (str or path): Root of resource directory.\n",
        "        dataset_name (str): Name of the training/validation dataset containing\n",
        "                              structured folders for image, label\n",
        "        usage (str): Either 'train' or 'validation'.\n",
        "        transform (list): Each element is string name of the transformation to\n",
        "            be used.\n",
        "        \"\"\"\n",
        "        self.src_dir = src_dir\n",
        "        self.dataset_name = dataset_name\n",
        "        self.csv_name = csv_name\n",
        "        self.apply_normalization = apply_normalization\n",
        "        self.transform = transform\n",
        "        self.patch_size = patch_size\n",
        "        self.overlap = overlap\n",
        "\n",
        "        self.usage = usage\n",
        "        assert self.usage in [\"train\", \"validation\", \"inference\"], \\\n",
        "            \"Usage is not recognized.\"\n",
        "\n",
        "        if self.usage in [\"train\", \"validation\"]:\n",
        "            assert self.dataset_name is not None\n",
        "            img_dir = Path(src_dir) / self.dataset_name / self.usage / \"bands\"\n",
        "            img_fnames = [Path(dirpath) / f\n",
        "                          for (dirpath, dirnames, filenames) in os.walk(img_dir)\n",
        "                          for f in filenames if f.endswith(\".tif\")]\n",
        "            img_fnames.sort()\n",
        "\n",
        "            lbl_dir = Path(src_dir) / self.dataset_name / self.usage / \"labels\"\n",
        "            lbl_fnames = [Path(dirpath) / f\n",
        "                          for (dirpath, dirnames, filenames) in os.walk(lbl_dir)\n",
        "                          for f in filenames if f.endswith(\".tif\")]\n",
        "            lbl_fnames.sort()\n",
        "\n",
        "            self.img_chips = []\n",
        "            self.lbl_chips = []\n",
        "\n",
        "            for img_path, lbl_path in tqdm.tqdm(zip(img_fnames, lbl_fnames),\n",
        "                                                total=len(img_fnames)):\n",
        "                img_chip = load_data(\n",
        "                    img_path, is_label=False,\n",
        "                    apply_normalization=self.apply_normalization\n",
        "                )\n",
        "                img_chip = img_chip.transpose((1, 2, 0))\n",
        "\n",
        "                lbl_chip = load_data(lbl_path, is_label=True)\n",
        "\n",
        "                self.img_chips.append(img_chip)\n",
        "                self.lbl_chips.append(lbl_chip)\n",
        "\n",
        "            print('--------------{} patches cropped--------------'\\\n",
        "                  .format(len(self.img_chips)))\n",
        "\n",
        "        # This part handles prediction dataset\n",
        "        else:\n",
        "            assert self.csv_name is not None\n",
        "\n",
        "            ##### Add your code to read the \"csv\" file. (Expected 1 line)\n",
        "            catalog = pd.read_csv(os.path.join(self.src_dir, self.csv_name))\n",
        "\n",
        "            ##### use \"iloc\" and \"catalog_index\" to grab one line of catalog.\n",
        "            ##### (Expected 1 line)\n",
        "            self.catalog = catalog.iloc[catalog_index]\n",
        "\n",
        "            self.tile = (self.catalog[\"wrs_path\"], self.catalog[\"wrs_row\"])\n",
        "\n",
        "            img_path_ls = [self.catalog[\"img_dir\"]]\n",
        "            mask_path_ls = [self.catalog[\"mask_dir\"]]\n",
        "\n",
        "            self.meta = get_meta_from_bounds(Path(src_dir) / img_path_ls[0])\n",
        "\n",
        "            half_size = self.patch_size // 2\n",
        "\n",
        "            self.img_chips = []\n",
        "            self.coor = []\n",
        "\n",
        "            for img_path, mask_path in zip(img_path_ls, mask_path_ls):\n",
        "\n",
        "                ###### Add your code to load the image and assign it to a\n",
        "                ###### variable called \"img\".\n",
        "                ###### Use the \"load_data\" function, provided in the utility\n",
        "                ###### function. (Expected 1 line)\n",
        "                img = load_data(os.path.join(self.src_dir, img_path),\n",
        "                                is_label = False,\n",
        "                                apply_normalization = self.apply_normalization)\n",
        "\n",
        "                img = np.transpose(img, (1, 2, 0))\n",
        "\n",
        "                ##### Load your mask again using \"load_data\" function.\n",
        "                ##### (Expected 1 line)\n",
        "                mask = load_data(os.path.join(self.src_dir, mask_path),\n",
        "                                 is_label=True)\n",
        "\n",
        "                crop_ref = mask\n",
        "\n",
        "                index = patch_center_index(crop_ref, self.patch_size,\n",
        "                                           self.overlap, self.usage)\n",
        "\n",
        "                for i in range(len(index)):\n",
        "                    x = index[i][0]\n",
        "                    y = index[i][1]\n",
        "\n",
        "                    self.img_chips.append(img[x - half_size: x + half_size,\n",
        "                                              y - half_size: y + half_size, :])\n",
        "                    self.coor.append([x, y])\n",
        "\n",
        "\n",
        "\n",
        "            print('--------------{} patches cropped--------------'\\\n",
        "                  .format(len(self.img_chips)))\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        if self.usage in [\"train\", \"validation\"]:\n",
        "            image_chip = self.img_chips[index]\n",
        "            label_chip = self.lbl_chips[index]\n",
        "\n",
        "            if self.usage == \"train\" and self.transform:\n",
        "                trans_flip_ls = [m for m in self.transform if \"flip\" in m]\n",
        "                if random.randint(0, 1) and len(trans_flip_ls) > 1:\n",
        "                    trans_flip = random.sample(trans_flip_ls, 1)[0]\n",
        "                    image_chip, label_chip = flip_image_and_label(\n",
        "                        image_chip, label_chip, trans_flip\n",
        "                    )\n",
        "\n",
        "                if random.randint(0, 1) and \"rotate\" in self.transform:\n",
        "                    img_chip, lbl_chip = rotate_image_and_label(\n",
        "                        image_chip, label_chip, angle=[0,90]\n",
        "                    )\n",
        "\n",
        "            # Convert numpy arrays to torch tensors.\n",
        "            # Image chips should be: CHW if not transpose to correct order of\n",
        "            # dimensions.\n",
        "            image_tensor = torch.from_numpy(image_chip.transpose((2, 0, 1)))\\\n",
        "                .float()\n",
        "            label_tensor = torch.from_numpy(np.ascontiguousarray(label_chip))\\\n",
        "                .long()\n",
        "\n",
        "            return image_tensor, label_tensor\n",
        "        else:\n",
        "            coor = self.coor[index]\n",
        "            img_chip = self.img_chips[index]\n",
        "            image_tensor = torch.from_numpy(img_chip.transpose((2, 0, 1)))\\\n",
        "                .float()\n",
        "\n",
        "            return image_tensor, coor\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_chips)"
      ],
      "metadata": {
        "id": "T3JVuBcVqYsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Building\n",
        "Deeplab3+ based on [Chen et al., 2024](https://link.springer.com/content/pdf/10.1007/s40747-023-01304-z.pdf)\n"
      ],
      "metadata": {
        "id": "93Q9eXHW94ng"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iv7azLGq97cB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "\n",
        "Chen LC et al., (2018)\n",
        "Encoder-decoder with atrous separable convolution for semantic\n",
        "image segmentation, In: Proceedings of the European conference\n",
        "on computer vision (ECCV). 801â€“818"
      ],
      "metadata": {
        "id": "ILQecsuySmxC"
      }
    }
  ]
}