{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Objective3:\n",
        "# DeepLab3+ Model\n",
        "DeepLabv3+ utilizes an encoder-decoder structure to perform image segmentation. The encoder extracts shallow and high-level semantic information from the image, while the decoder combines low-level and high-level features to improve the accuracy of segmentation boundaries and classify the semantic information of different pixels [Chen et al., (2018)](https://link.springer.com/content/pdf/10.1007/978-3-030-01234-2_49.pdf?pdf=inline%20link).\n",
        "\n",
        "This project is based on the improved classis DeepLabv3+ network model proposed by [Chen et al.,(2023)](https://link.springer.com/content/pdf/10.1007/s40747-023-01304-z.pdf).\n",
        "\n",
        "## Architecture of improved DeepLabv3+ with MobileNetv2 backbone\n",
        "\n",
        "**`A. Encoder`**\n",
        " 1. `Backbone` : lightweight network `MobileNetv2` [Sandler et al., (2019)](https://arxiv.org/pdf/2111.12419) in place of Xception.\n",
        " 2. `ASPP` : `Hybrid Dialted Convolution` (HDC) module to alleviate the gridding effect. In addition,  `Strip Pooling Module` is used instead of spatial mean pooling to improve th elocal segmentation effect.\n",
        " 3. `Normalization-based Attention Module` (NAM): This lightweight attention mechanism is also applied to the stacked compressed high-level feature maps to help improve the segmentation accuracy of the image.\n",
        "\n",
        "**`B. Decoder`**\n",
        "1. `NAM`: The seventh layer feature with `NAM` attention [Liu et al., (2021)](https://arxiv.org/pdf/1801.04381v4) is upsampled to the same size as the fourth layer feature after fusion and channel adjustment.\n",
        "2. `ResNet50`: This module is added to obtain riccher low-level target feature information.\n",
        "3. `Concatenate`: The **deep features** and **shallow features** are concatenated as in the original model.\n",
        "4. `Upsampling`: After a 3 X 3 convolution and 4 X `upsampling`, the image is restored to its original size.\n",
        "\n",
        "\n",
        " [Architecture Image]()"
      ],
      "metadata": {
        "id": "Av8BX6v5mL_s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data Preparation"
      ],
      "metadata": {
        "id": "-byRvwK76QOk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ehcJSZG5ad6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3a682bc-fcd1-4c44-ac7c-0428d5ebc51f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install required packages\n",
        "\n",
        "%%capture\n",
        "!pip install rasterio\n"
      ],
      "metadata": {
        "id": "TB_rgHf166Qg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required packages\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "import tqdm # Adds a smart progress meter to any iterable or file operation\n",
        "\n",
        "import math\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import cv2\n",
        "import rasterio\n",
        "#  defines a rectangular area within the raster using four properties\n",
        "# xoff, yoff, width, height\n",
        "from rasterio.windows import Window\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import torchvision.models as pretrainmodels\n",
        "\n",
        "import logging\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "import itertools\n",
        "\n",
        "\n",
        "from IPython.core.debugger import set_trace # Insert a breakpoint into the code\n",
        "from IPython.display import Image\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "XNc4dQUl7RAQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions"
      ],
      "metadata": {
        "id": "4FzjUpiAg00A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InputError(Exception):\n",
        "    '''\n",
        "    Exception raised for errors in the input\n",
        "    '''\n",
        "    def __init__(self, message):\n",
        "        '''\n",
        "        Params:\n",
        "            message (str): explanation of the error\n",
        "        '''\n",
        "        self.message = message\n",
        "    def __str__(self):\n",
        "        '''\n",
        "        Define message to return when error is raised\n",
        "        '''\n",
        "        if self.message:\n",
        "            return 'InputError, {} '.format(self.message)\n",
        "        else:\n",
        "            return 'InputError'\n",
        "# =============================================================================\n",
        "def load_data(data_path, usage=\"train\", window=None, norm_stats_type=None,\n",
        "              is_label=False):\n",
        "    '''\n",
        "    Read geographic data into numpy array\n",
        "    Params:\n",
        "        data_path : str\n",
        "            Path of data to load\n",
        "        usage : str\n",
        "            Usage of the data: \"train\", \"validate\", or \"predict\"\n",
        "        window : tuple\n",
        "            The view onto a rectangular subset of the data, in the format of\n",
        "            (column offsets, row offsets, width in pixel, height in pixel)\n",
        "        norm_stats_type : str\n",
        "            How the normalization statistics is calculated.\n",
        "        is_label : binary\n",
        "            Decide whether to saturate data with tested threshold\n",
        "    Returns:\n",
        "        narray\n",
        "    '''\n",
        "    # Open the data file using the 'rasterio' library\n",
        "    with rasterio.open(data_path, \"r\") as src:\n",
        "      # Check if the data is a label (segmentation mask)\n",
        "        if is_label:\n",
        "            if src.count != 1:  # Ensure the label has a single channel\n",
        "                raise InputError(\"Label shape not applicable: \\\n",
        "                                expected 1 channel\")\n",
        "            img = src.read(1)  # Read the single channel of the label data\n",
        "\n",
        "        else:\n",
        "        # Store the value representing 'no data' in the image\n",
        "            nodata = src.nodata\n",
        "            # Verify normalization type is valid\n",
        "            assert norm_stats_type in [\"local_per_tile\", \"local_per_band\",\n",
        "                                      \"global_per_band\"]\n",
        "\n",
        "            if norm_stats_type == \"local_per_tile\":\n",
        "              # Apply per-tile normalization\n",
        "                img = mmnorm1(src.read(), nodata=nodata)\n",
        "            elif norm_stats_type == \"local_per_band\":\n",
        "              # Per-band normalization, clipping values\n",
        "                img = mmnorm2(src.read(), nodata=nodata, clip_val=1.5)\n",
        "            elif norm_stats_type == \"global_per_band\":\n",
        "              # Global per-band normalization, clipping values\n",
        "                img = mmnorm3(src.read(), nodata=nodata, clip_val=1.5)\n",
        "\n",
        "            # For 'train' or 'validate' subsets\n",
        "            if usage in ['train', 'validate']:\n",
        "              # Extract a specific window from the image\n",
        "                img = img[:, max(0, window[1]): window[1] + window[3],\n",
        "                          max(0, window[0]): window[0] + window[2]]\n",
        "\n",
        "    return img  # Return the processed image or label data\n",
        "# ==============================================================================\n",
        "\n",
        "def get_stacked_img(img_paths, usage, norm_stats_type=\"local_per_tile\",\n",
        "                    window=None):\n",
        "    '''\n",
        "    Read geographic data into numpy array\n",
        "    Params:\n",
        "        gsPath :str\n",
        "            Path of growing season image\n",
        "        osPath : str\n",
        "            Path of off season image\n",
        "        img_paths : list\n",
        "            List of paths for imgages\n",
        "        usage : str\n",
        "            Usage of the image: \"train\", \"validate\", or \"predict\"\n",
        "        norm_stats_type : str\n",
        "            How the normalization statistics is calculated.\n",
        "        window : tuple\n",
        "            The view onto a rectangular subset of the data, in the\n",
        "            format of (column offsets, row offsets, width in pixel, height in\n",
        "            pixel)\n",
        "    Returns:\n",
        "        ndarray\n",
        "    '''\n",
        "\n",
        "    if len(img_paths) > 1:  # If there are multiple image paths:\n",
        "      img_ls = [load_data(m, usage, window, norm_stats_type) for m in img_paths]\n",
        "      # Load data for each image path, potentially applying normalization\n",
        "      img = np.concatenate(img_ls, axis=0).transpose(1, 2, 0)\n",
        "      # Combine the loaded data into a single array and rearrange dimensions\n",
        "    else:  # If there's only a single image path:\n",
        "      # Load data for the single image path and rearrange dimensions\n",
        "      img = load_data(img_paths[0], usage, \\\n",
        "                      window, norm_stats_type).transpose(1, 2, 0)\n",
        "\n",
        "    # For 'train' or 'validate' subsets:\n",
        "    if usage in [\"train\", \"validate\"]:\n",
        "      # Extract window parameters\n",
        "      col_off, row_off, col_target, row_target = window\n",
        "      row, col, c = img.shape  # Get image dimensions\n",
        "\n",
        "      # Check if image is smaller than the target window\n",
        "      if row < row_target or col < col_target:\n",
        "          row_off = abs(row_off) if row_off < 0 else 0  # Adjust offsets\n",
        "          col_off = abs(col_off) if col_off < 0 else 0\n",
        "\n",
        "          # Create a larger blank canvas\n",
        "          canvas = np.zeros((row_target, col_target, c))\n",
        "          # Place image onto canvas\n",
        "          canvas[row_off: row_off + row, col_off : col_off + col, :] = img\n",
        "          return canvas  # Return the canvas with the padded image\n",
        "\n",
        "      else:\n",
        "          return img  # The image fits the window, so return it directly\n",
        "\n",
        "    elif usage == \"predict\":  # For prediction purposes:\n",
        "      return img  # Return the image as is\n",
        "\n",
        "    else:\n",
        "      raise ValueError  # Invalid 'usage' value\n",
        "\n",
        "# ==============================================================================\n",
        "def get_buffered_window(src_path, dst_path, buffer):\n",
        "    '''\n",
        "    Get bounding box representing subset of source image that overlaps with\n",
        "    bufferred destination image, in format of (column offsets, row offsets,\n",
        "    width, height)\n",
        "\n",
        "    Params:\n",
        "        src_path : str\n",
        "            Path of source image to get subset bounding box\n",
        "        dst_path : str\n",
        "            Path of destination image as a reference to define the\n",
        "            bounding box. Size of the bounding box is\n",
        "            (destination width + buffer * 2, destination height + buffer * 2)\n",
        "        buffer :int\n",
        "            Buffer distance of bounding box edges to destination image\n",
        "            measured by pixel numbers\n",
        "\n",
        "    Returns:\n",
        "        tuple in form of (column offsets, row offsets, width, height)\n",
        "    '''\n",
        "\n",
        "    with rasterio.open(src_path, \"r\") as src:\n",
        "        gt_src = src.transform\n",
        "\n",
        "    with rasterio.open(dst_path, \"r\") as dst:\n",
        "        gt_dst = dst.transform\n",
        "        w_dst = dst.width\n",
        "        h_dst = dst.height\n",
        "\n",
        "    col_off = round((gt_dst[2] - gt_src[2]) / gt_src[0]) - buffer\n",
        "    row_off = round((gt_dst[5] - gt_src[5]) / gt_src[4]) - buffer\n",
        "    width = w_dst + buffer * 2\n",
        "    height = h_dst + buffer * 2\n",
        "\n",
        "    return col_off, row_off, width, height\n",
        "\n",
        "# ==============================================================================\n",
        "\n",
        "def get_meta_from_bounds(file, buffer):\n",
        "    '''\n",
        "    Get metadata of unbuffered region in given file\n",
        "    Params:\n",
        "        file (str):  File name of a image chip\n",
        "        buffer (int): Buffer distance measured by pixel numbers\n",
        "    Returns:\n",
        "        dictionary\n",
        "    '''\n",
        "\n",
        "    with rasterio.open(file, \"r\") as src:\n",
        "\n",
        "        meta = src.meta\n",
        "        dst_width = src.width - 2 * buffer\n",
        "        dst_height = src.height - 2 * buffer\n",
        "\n",
        "        window = Window(buffer, buffer, dst_width, dst_height)\n",
        "        win_transform = src.window_transform(window)\n",
        "\n",
        "    meta.update({\n",
        "        'width': dst_width,\n",
        "        'height': dst_height,\n",
        "        'transform': win_transform,\n",
        "        'count': 1,\n",
        "        'nodata': -128,\n",
        "        'dtype': 'int8'\n",
        "    })\n",
        "\n",
        "    return meta\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "def display_hist(img):\n",
        "    '''\n",
        "    Display data distribution of input image in a histogram\n",
        "    Params:\n",
        "        img (narray): Image in form of (H,W,C) to display data distribution\n",
        "    '''\n",
        "\n",
        "    img = mmnorm1(img)\n",
        "    im = np.where(img == 0, np.nan, img)\n",
        "\n",
        "    plt.hist(img.ravel(), 500, [np.nanmin(im), img.max()])\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    plt.show()\n",
        "\n",
        "# ==============================================================================\n",
        "def mmnorm1(img, nodata):\n",
        "    '''\n",
        "    Data normalization with min/max method\n",
        "    Params:\n",
        "        img (narray): The targeted image for normalization\n",
        "    Returns:\n",
        "        narrray\n",
        "    '''\n",
        "\n",
        "    img_tmp = np.where(img == nodata, np.nan, img)\n",
        "    img_max = np.nanmax(img_tmp)\n",
        "    img_min = np.nanmin(img_tmp)\n",
        "    normalized = (img - img_min) / (img_max - img_min)\n",
        "    normalized = np.clip(normalized, 0, 1)\n",
        "\n",
        "    return normalized\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "def mmnorm2(img, nodata, clip_val=None):\n",
        "    r\"\"\"\n",
        "    Normalize the input image pixels to [0, 1] ranged based on the\n",
        "    minimum and maximum statistics of each band per tile.\n",
        "    Arguments:\n",
        "            img : numpy array\n",
        "                Stacked image bands with a dimension of (C,H,W).\n",
        "            nodata : str\n",
        "                Value reserved to represent NoData in the image chip.\n",
        "            clip_val : int\n",
        "                Defines how much of the distribution tails to be cut off.\n",
        "    Returns:\n",
        "            img : numpy array\n",
        "                Normalized image stack of size (C,H,W).\n",
        "    Note 1: If clip then min, max are calculated from the clipped image.\n",
        "    \"\"\"\n",
        "\n",
        "    # filter out zero pixels in generating statistics.\n",
        "    nan_corr_img = np.where(img == nodata, np.nan, img)\n",
        "    nan_corr_img = np.where(img == 0, np.nan, img)\n",
        "\n",
        "    if clip_val > 0:\n",
        "        left_tail_clip = np.nanpercentile(nan_corr_img, clip_val)\n",
        "        right_tail_clip = np.nanpercentile(nan_corr_img, 100 - clip_val)\n",
        "\n",
        "        left_clipped_img = np.where(img < left_tail_clip, left_tail_clip, img)\n",
        "        clipped_img = np.where(left_clipped_img > right_tail_clip,\n",
        "                               right_tail_clip, left_clipped_img)\n",
        "\n",
        "        normalized_bands = []\n",
        "        for i in range(img.shape[0]):\n",
        "            band_min = np.nanmin(clipped_img[i, :, :])\n",
        "            band_max = np.nanmax(clipped_img[i, :, :])\n",
        "            normalized_band = (clipped_img[i, :, :] - band_min) /\\\n",
        "                (band_max - band_min)\n",
        "            normalized_bands.append(np.expand_dims(normalized_band, 0))\n",
        "        normal_img = np.concatenate(normalized_bands, 0)\n",
        "\n",
        "    elif clip_val == 0 or clip_val is None:\n",
        "        normalized_bands = []\n",
        "        for i in range(img.shape[0]):\n",
        "            band_min = np.nanmin(nan_corr_img[i, :, :])\n",
        "            band_max = np.nanmax(nan_corr_img[i, :, :])\n",
        "            normalized_band = (nan_corr_img[i, :, :] - band_min) /\\\n",
        "                (band_max - band_min)\n",
        "            normalized_bands.append(np.expand_dims(normalized_band, 0))\n",
        "        normal_img = np.concatenate(normalized_bands, 0)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"clip must be a non-negative decimal.\")\n",
        "\n",
        "    normal_img = np.clip(normal_img, 0, 1)\n",
        "    return normal_img\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "def mmnorm3(img, nodata, clip_val=None):\n",
        "    hardcoded_stats = {\n",
        "        \"mins\": np.array([331.0, 581.0, 560.0, 1696.0]),\n",
        "        \"maxs\": np.array([1403.0, 1638.0, 2076.0, 3652.0])\n",
        "    }\n",
        "\n",
        "    num_bands = img.shape[0]\n",
        "    mins = hardcoded_stats[\"mins\"]\n",
        "    maxs = hardcoded_stats[\"maxs\"]\n",
        "\n",
        "    if clip_val:\n",
        "        normalized_bands = []\n",
        "        for i in range(num_bands):\n",
        "            nan_corr_img = np.where(img[i, :, :] == nodata, np.nan,\n",
        "                                    img[i, :, :])\n",
        "            nan_corr_img = np.where(img[i, :, :] == 0, np.nan, img[i, :, :])\n",
        "            left_tail_clip = np.nanpercentile(nan_corr_img, clip_val)\n",
        "            right_tail_clip = np.nanpercentile(nan_corr_img, 100 - clip_val)\n",
        "            left_clipped_band = np.where(img[i, :, :] < left_tail_clip,\n",
        "                                         left_tail_clip, img[i, :, :])\n",
        "            clipped_band = np.where(left_clipped_band > right_tail_clip,\n",
        "                                    right_tail_clip, left_clipped_band)\n",
        "            normalized_band = (clipped_band - mins[i]) / (maxs[i] - mins[i])\n",
        "            normalized_bands.append(np.expand_dims(normalized_band, 0))\n",
        "        img = np.concatenate(normalized_bands, 0)\n",
        "\n",
        "    else:\n",
        "        for i in range(num_bands):\n",
        "            img[i, :, :] = (img[i, :, :] - mins[i]) / (maxs[i] - mins[i])\n",
        "\n",
        "    img = np.clip(img, 0, 1)\n",
        "    return img\n",
        "\n",
        "# ==============================================================================\n",
        "def get_chips(img, dsize, buffer):\n",
        "    '''\n",
        "    Generate small chips from input images and the corresponding index of each\n",
        "    chip The index marks the location of corresponding upper-left pixel of a\n",
        "    chip.\n",
        "    Params:\n",
        "        img (narray): Image in format of (H,W,C) to be crop, in this case it is\n",
        "            the concatenated image of growing season and off season\n",
        "        dsize (int): Cropped chip size\n",
        "        buffer (int):Number of overlapping pixels when extracting images chips\n",
        "    Returns:\n",
        "        list of cropped chips and corresponding coordinates\n",
        "    '''\n",
        "\n",
        "    h, w, _ = img.shape\n",
        "    x_ls = range(0,h - 2 * buffer, dsize - 2 * buffer)\n",
        "    y_ls = range(0, w - 2 * buffer, dsize - 2 * buffer)\n",
        "\n",
        "    index = list(itertools.product(x_ls, y_ls))\n",
        "\n",
        "    img_ls = []\n",
        "    for i in range(len(index)):\n",
        "        x, y = index[i]\n",
        "        img_ls.append(img[x:x + dsize, y:y + dsize, :])\n",
        "\n",
        "    return img_ls, index\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "def display(img, label, mask):\n",
        "\n",
        "    '''\n",
        "    Display composites and their labels\n",
        "    Params:\n",
        "        img (torch.tensor): Image in format of (C,H,W)\n",
        "        label (torch.tensor): Label in format of (H,W)\n",
        "        mask (torch.tensor): Mask in format of (H,W)\n",
        "    '''\n",
        "\n",
        "    gsimg = (comp432_dis(img, \"GS\") * 255).permute(1, 2, 0).int()\n",
        "    osimg = (comp432_dis(img, \"OS\") * 255).permute(1, 2, 0).int()\n",
        "\n",
        "\n",
        "    _, figs = plt.subplots(1, 4, figsize=(20, 20))\n",
        "\n",
        "    label = label.cpu()\n",
        "\n",
        "    figs[0].imshow(gsimg)\n",
        "    figs[1].imshow(osimg)\n",
        "    figs[2].imshow(label)\n",
        "    figs[3].imshow(mask)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# ==============================================================================\n",
        "# color composite\n",
        "def comp432_dis(img, season):\n",
        "    '''\n",
        "    Generate false color composites\n",
        "    Params:\n",
        "        img (torch.tensor): Image in format of (C,H,W)\n",
        "        season (str): Season of the composite to generate, be  \"GS\" or \"OS\"\n",
        "    '''\n",
        "\n",
        "    viewsize = img.shape[1:]\n",
        "\n",
        "    if season == \"GS\":\n",
        "\n",
        "        b4 = mmnorm1(img[3, :, :].cpu().view(1, *viewsize),0)\n",
        "        b3 = mmnorm1(img[2, :, :].cpu().view(1, *viewsize),0)\n",
        "        b2 = mmnorm1(img[1, :, :].cpu().view(1, *viewsize),0)\n",
        "\n",
        "    elif season == \"OS\":\n",
        "        b4 = mmnorm1(img[7, :, :].cpu().view(1, *viewsize), 0)\n",
        "        b3 = mmnorm1(img[6, :, :].cpu().view(1, *viewsize), 0)\n",
        "        b2 = mmnorm1(img[5, :, :].cpu().view(1, *viewsize), 0)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Bad season value\")\n",
        "\n",
        "    img = torch.cat([b4, b3, b2], 0)\n",
        "\n",
        "    return img\n",
        "\n",
        "# ==============================================================================\n",
        "def make_reproducible(seed=42, cudnn=True):\n",
        "    \"\"\"Make all the randomization processes start from a shared seed\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.random.manual_seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    if cudnn:\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "# ==============================================================================\n",
        "\n",
        "def pickle_dataset(dataset, file_path):\n",
        "    with open(file_path, \"wb\") as fp:\n",
        "              pickle.dump(dataset, fp)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "def load_pickle(file_path):\n",
        "    return pd.read_pickle(file_path)\n",
        "\n",
        "# ==============================================================================\n",
        "def progress_reporter(msg, verbose, logger=None):\n",
        "    \"\"\"Helps control print statements and log writes\n",
        "    Parameters\n",
        "    ----------\n",
        "    msg : str\n",
        "      Message to write out\n",
        "    verbose : bool\n",
        "      Prints or not to console\n",
        "    logger : logging.logger\n",
        "      logger (defaults to none)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "        Message to console and or log\n",
        "    \"\"\"\n",
        "\n",
        "    if verbose:\n",
        "        print(msg)\n",
        "\n",
        "    if logger:\n",
        "        logger.info(msg)\n",
        "\n",
        "# ==============================================================================\n",
        "def setup_logger(log_dir, log_name, use_date=False):\n",
        "    \"\"\"Create logger\n",
        "    \"\"\"\n",
        "    if use_date:\n",
        "        dt = datetime.now().strftime(\"%d%m%Y_%H%M\")\n",
        "        log = \"{}/{}_{}.log\".format(log_dir, log_name, dt)\n",
        "    else:\n",
        "        log = \"{}/{}.log\".format(log_dir, log_name)\n",
        "\n",
        "    for handler in logging.root.handlers[:]:\n",
        "        logging.root.removeHandler(handler)\n",
        "    log_format = (\n",
        "        f\"%(asctime)s::%(levelname)s::%(name)s::%(filename)s::\"\n",
        "        f\"%(lineno)d::%(message)s\"\n",
        "    )\n",
        "    logging.basicConfig(filename=log, filemode='w',\n",
        "                        level=logging.INFO, format=log_format)\n",
        "\n",
        "    return logging.getLogger()"
      ],
      "metadata": {
        "id": "7aXBJ3ALg0bW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation\n",
        "1. Find the suitable `image dataset` to apply `improved DeepLabv3+ model` for the image segmentation process.\n",
        "  - For this task, we used image dataset that was used in `S. Khallaghi, (2024) ch. 2`.\n",
        "2. Prepare the `labels (pixel-wise annotations)` that are compatible with selected image dataset.\n",
        "  - For this task, we filtered [all_class_cataloge](/content/gdrive/MyDrive/adleo/project_data/label_catalog_allclasses.csv) using methods and functions given in [notebook](https://github.com/paudelsushil/labelcombinations/blob/main/Make_Labels_ADLEO_Final.ipynb) and prepared final [filtered cataloge](/content/gdrive/MyDrive/adleo/project_data/label-catalog-filtered.csv) to get our pixel-wise annotations as a [lable images](/content/gdrive/MyDrive/adleo/project_data/labels).\n"
      ],
      "metadata": {
        "id": "pzxo77ik_UJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tirz3g1JIeAr",
        "outputId": "8d515836-eedb-4db0-9289-b6b540c8450d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Dataset for training, validating, and testing the model"
      ],
      "metadata": {
        "id": "rAQQOPVvBqAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the datasets source and workingFolder source\n",
        "src_dir = \"/content/gdrive/MyDrive/adleo/project_data\"\n",
        "\n",
        "WorkingFolder = \"/content/gdrive/MyDrive/adleo/project_data\"\n",
        "\n",
        "\n",
        "# Define the path for images, lables, and cataloge\n",
        "# Image path\n",
        "img_paths = list(Path(os.path.join(src_dir, \"images\")).glob(\"*.tif\"))\n",
        "\n",
        "# Label path\n",
        "lbl_paths = list(Path(os.path.join(src_dir, \"labels\")).glob(\"*.tif\"))\n",
        "\n",
        "# Label cataloge\n",
        "cataloge = pd.read_csv(os.path.join(src_dir, \"label-catalog-filtered.csv\"))\n",
        "\n",
        "# Check if all paths are valid lists\n",
        "if not all(isinstance(path_list, list) for path_list in (img_paths, lbl_paths)):\n",
        "    raise ValueError(\"Both image_paths and label_paths must be lists.\")\n",
        "\n",
        "# Prints valid number of images and labels\n",
        "print(\"No. of images:\",len(img_paths), \"\\n\",\n",
        "      \"No. of labels:\", len(lbl_paths),\"\\n\",\n",
        "      \"No. of rows in cataloge:\", len(cataloge))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MA9ZtQWP93z5",
        "outputId": "5cac22cc-6535-4da4-895e-d6b8ec7542ec"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-f3a55325f6a4>:15: DtypeWarning: Columns (2,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  cataloge = pd.read_csv(os.path.join(src_dir, \"label-catalog-filtered.csv\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of images: 33873 \n",
            " No. of labels: 33756 \n",
            " No. of rows in cataloge: 33746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-process Datasets:\n",
        "1. `Resize images` to a standard size suitable for model.\n",
        "\n",
        "2. `Normalize pixel values`(e.g., scale to range 0-1 or subtract mean).\n",
        "\n",
        "3. `Image Augmentation`  (e.g., random flipping, cropping).\n"
      ],
      "metadata": {
        "id": "xBHQIHCUCBrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image Normalization"
      ],
      "metadata": {
        "id": "upnS_NL-Cfoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def min_max_normalize_image(image, dtype=np.float32):\n",
        "    \"\"\"\n",
        "    image_path(str) : Absolute path to the image patch.\n",
        "    dtype (numpy datatype) : data type of the normalized image default is\n",
        "    \"np.float32\".\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the minimum and maximum values for each band\n",
        "    min_values = np.nanmin(image, axis=(1, 2))[:, np.newaxis, np.newaxis]\n",
        "    max_values = np.nanmax(image, axis=(1, 2))[:, np.newaxis, np.newaxis]\n",
        "\n",
        "    # Normalize the image data to the range [0, 1]\n",
        "    normalized_img = (image - min_values) / (max_values - min_values)\n",
        "\n",
        "    # Return the normalized image data\n",
        "    return normalized_img"
      ],
      "metadata": {
        "id": "cGI_pLF0Ce6Z"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image Augmentation"
      ],
      "metadata": {
        "id": "tj_af9t_Cr_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flip_image_and_label(image, label, flip_type):\n",
        "    \"\"\"\n",
        "    Applies horizontal or vertical flip augmentation to an image patch and label\n",
        "\n",
        "    Args:\n",
        "        image (numpy array) : The input image patch as a numpy array.\n",
        "        label (numpy array) : The corresponding label as a numpy array.\n",
        "        flip_type (string) : Based on the direction of flip. Can be either\n",
        "            'hflip' or 'vflip'.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the flipped image patch and label as numpy arrays.\n",
        "    \"\"\"\n",
        "    if flip_type == 'hflip':\n",
        "        # Apply horizontal flip augmentation to the image patch\n",
        "        flipped_image = cv2.flip(image, 1)\n",
        "\n",
        "        # Apply horizontal flip augmentation to the label\n",
        "        flipped_label = cv2.flip(label, 1)\n",
        "\n",
        "    elif flip_type == 'vflip':\n",
        "        # Apply vertical flip augmentation to the image patch\n",
        "        flipped_image = cv2.flip(image, 0)\n",
        "\n",
        "        # Apply vertical flip augmentation to the label\n",
        "        flipped_label = cv2.flip(label, 0)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Flip direction must be 'horizontal' or 'vertical'.\")\n",
        "\n",
        "    # Return the flipped image patch and label as a tuple\n",
        "    return flipped_image.copy(), flipped_label.copy()\n",
        "\n",
        "\n",
        "def rotate_image_and_label(image, label, angle):\n",
        "    \"\"\"\n",
        "    Applies rotation augmentation to an image patch and label.\n",
        "\n",
        "    Args:\n",
        "        image (numpy array) : The input image patch as a numpy array.\n",
        "        label (numpy array) : The corresponding label as a numpy array.\n",
        "        angle (lost of floats) : If the list has exactly two elements they will\n",
        "            be considered the lower and upper bounds for the rotation angle\n",
        "            (in degrees) respectively. If number of elements are bigger than 2,\n",
        "            then one value is chosen randomly as the roatation angle.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the rotated image patch and label as numpy arrays.\n",
        "    \"\"\"\n",
        "    if isinstance(angle, tuple) or isinstance(angle, list):\n",
        "        if len(angle) == 2:\n",
        "            rotation_degree = random.uniform(angle[0], angle[1])\n",
        "        elif len(angle) > 2:\n",
        "            rotation_degree = random.choice(angle)\n",
        "        else:\n",
        "            raise ValueError(\"Parameter degree needs at least two elements.\")\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"Rotation bound param for augmentation must be a tuple or list.\"\n",
        "        )\n",
        "\n",
        "    # Define the center of the image patch\n",
        "    center = tuple(np.array(label.shape)/2.0)\n",
        "\n",
        "    # Define the rotation matrix\n",
        "    rotation_matrix = cv2.getRotationMatrix2D(center, rotation_degree, 1.0)\n",
        "\n",
        "    # Apply rotation augmentation to the image patch\n",
        "    rotated_image = cv2.warpAffine(image, rotation_matrix, image.shape[:2],\n",
        "                                   flags=cv2.INTER_LINEAR)\n",
        "\n",
        "    # Apply rotation augmentation to the label\n",
        "    rotated_label = cv2.warpAffine(label, rotation_matrix, label.shape[:2],\n",
        "                                   flags=cv2.INTER_NEAREST)\n",
        "\n",
        "    # Return the rotated image patch and label as a tuple\n",
        "    return rotated_image.copy(), np.rint(rotated_label.copy())"
      ],
      "metadata": {
        "id": "lFJyV0MyCMe5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get center index fo each smaller chips"
      ],
      "metadata": {
        "id": "Kj5Uw-TukYEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def patch_center_index(cropping_ref, patch_size, overlap, usage,\n",
        "                       positive_class_threshold=None, verbose=True):\n",
        "    \"\"\"\n",
        "    Generate index to divide the scene into small chips.\n",
        "    Each index marks the location of corresponding chip center.\n",
        "    Arguments:\n",
        "        cropping_ref (list) : Reference raster layers, to be used to generate\n",
        "            the index. In our case, it is study area binary mask and label mask.\n",
        "        patch_size (int) : Size of each clipped patches.\n",
        "        overlap (int) : amount of overlap between the extracted chips.\n",
        "        usage (str) : Either 'train', 'val'. Chipping strategy is different for\n",
        "            different usage.\n",
        "        positive_class_threshold (float) : A real value as a threshold for the\n",
        "            proportion of positive class to the total areal of the chip. Used to\n",
        "            decide if the chip should be considered as a positive chip in the\n",
        "            sampling process.\n",
        "    verbose (binary) : If set to True prints on screen the detailed list of\n",
        "            center coordinates of the sampled chips.\n",
        "    Returns:\n",
        "        proportional_patch_index : A list of index recording the center of\n",
        "        patches to extract from the input\n",
        "    \"\"\"\n",
        "\n",
        "    assert usage in [\"train\", \"validation\", \"inference\"]\n",
        "\n",
        "    if usage == \"inference\":\n",
        "        mask = cropping_ref\n",
        "    else:\n",
        "        mask, label = cropping_ref\n",
        "\n",
        "    half_size = patch_size // 2\n",
        "    step_size = patch_size - 2 * overlap\n",
        "\n",
        "    proportional_patch_index = []\n",
        "    non_proportional_patch_index = []\n",
        "    neg_patch_index = []\n",
        "\n",
        "    # Get the index of all the non-zero elements in the mask.\n",
        "    x = np.argwhere(mask)\n",
        "\n",
        "    # First col of x shows the row indices (height) of the mask layer\n",
        "    # (iterate over the y axis or latitude).\n",
        "    x_min = min(x[:, 0]) + half_size\n",
        "    x_max = max(x[:, 0]) - half_size\n",
        "    # Second col of x shows the column indices (width) of the mask layer\n",
        "    # (iterate over the x axis or longitude).\n",
        "    y_min = min(x[:, 1]) + half_size\n",
        "    y_max = max(x[:, 1]) - half_size\n",
        "\n",
        "    # Generate index for the center of each patch considering the proportion of\n",
        "    # each category falling into each patch.\n",
        "    for j in range(y_min, y_max + 1, step_size):\n",
        "\n",
        "        for i in range(x_min, x_max + 1, step_size):\n",
        "\n",
        "            # Split the mask and label layers into patches based on the index of\n",
        "            # the center of the patch\n",
        "            mask_ref = mask[i - half_size: i + half_size,\n",
        "                            j - half_size: j + half_size]\n",
        "            if usage != \"inference\":\n",
        "                label_ref = label[i - half_size: i + half_size,\n",
        "                                  j - half_size: j + half_size]\n",
        "\n",
        "            if (usage == \"train\") and mask_ref.all():\n",
        "\n",
        "                if label_ref.any() != 0:\n",
        "                    pond_ratio = np.sum(label_ref == 1) / label_ref.size\n",
        "                    if pond_ratio >= positive_class_threshold:\n",
        "                        proportional_patch_index.append([i, j])\n",
        "                else:\n",
        "                    neg_patch_index.append([i, j])\n",
        "\n",
        "            if (usage == \"validation\") and (label_ref.any() != 0) \\\n",
        "                and mask_ref.all():\n",
        "                non_proportional_patch_index.append([i, j])\n",
        "\n",
        "            if (usage == \"inference\") and (mask_ref.any() != 0):\n",
        "                non_proportional_patch_index.append([i, j])\n",
        "\n",
        "    if usage == \"train\":\n",
        "\n",
        "        num_negative_samples = min(\n",
        "            math.ceil(0.2 * len(proportional_patch_index)), 15\n",
        "        )\n",
        "        neg_samples = random.sample(neg_patch_index, num_negative_samples)\n",
        "\n",
        "        proportional_patch_index.extend(neg_samples)\n",
        "\n",
        "    # For test set use the indices generated from mask without\n",
        "    # considering the class proportions.\n",
        "    if usage in [\"validation\", \"inference\"]:\n",
        "        proportional_patch_index = non_proportional_patch_index\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Number of patches:\", len(proportional_patch_index))\n",
        "        print(\"Patched from:\\n{}\".format(proportional_patch_index))\n",
        "\n",
        "    return proportional_patch_index"
      ],
      "metadata": {
        "id": "qmXVg5omkXNT"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Active dataset loading pipeline\n"
      ],
      "metadata": {
        "id": "nYZ6dLjEqZvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class datasetloader(Dataset):\n",
        "    def __init__(self, src_dir, usage, dataset_name=None,\n",
        "                 apply_normalization=False, transform=None, csv_name=None,\n",
        "                 patch_size=None, overlap=None, catalog_index=None):\n",
        "        r\"\"\"\n",
        "        src_dir (str or path): Root of resource directory.\n",
        "        dataset_name (str): Name of the training/validation dataset containing\n",
        "                              structured folders for image, label\n",
        "        usage (str): Either 'train' or 'validation'.\n",
        "        transform (list): Each element is string name of the transformation to\n",
        "            be used.\n",
        "        \"\"\"\n",
        "        self.src_dir = src_dir\n",
        "        self.dataset_name = dataset_name\n",
        "        self.csv_name = csv_name\n",
        "        self.apply_normalization = apply_normalization\n",
        "        self.transform = transform\n",
        "        self.patch_size = patch_size\n",
        "        self.overlap = overlap\n",
        "\n",
        "        self.usage = usage\n",
        "        assert self.usage in [\"train\", \"validation\", \"inference\"], \\\n",
        "            \"Usage is not recognized.\"\n",
        "\n",
        "        if self.usage in [\"train\", \"validation\"]:\n",
        "            assert self.dataset_name is not None\n",
        "            img_dir = Path(src_dir) / self.dataset_name / self.usage / \"bands\"\n",
        "            img_fnames = [Path(dirpath) / f\n",
        "                          for (dirpath, dirnames, filenames) in os.walk(img_dir)\n",
        "                          for f in filenames if f.endswith(\".tif\")]\n",
        "            img_fnames.sort()\n",
        "\n",
        "            lbl_dir = Path(src_dir) / self.dataset_name / self.usage / \"labels\"\n",
        "            lbl_fnames = [Path(dirpath) / f\n",
        "                          for (dirpath, dirnames, filenames) in os.walk(lbl_dir)\n",
        "                          for f in filenames if f.endswith(\".tif\")]\n",
        "            lbl_fnames.sort()\n",
        "\n",
        "            self.img_chips = []\n",
        "            self.lbl_chips = []\n",
        "\n",
        "            for img_path, lbl_path in tqdm.tqdm(zip(img_fnames, lbl_fnames),\n",
        "                                                total=len(img_fnames)):\n",
        "                img_chip = load_data(\n",
        "                    img_path, is_label=False,\n",
        "                    apply_normalization=self.apply_normalization\n",
        "                )\n",
        "                img_chip = img_chip.transpose((1, 2, 0))\n",
        "\n",
        "                lbl_chip = load_data(lbl_path, is_label=True)\n",
        "\n",
        "                self.img_chips.append(img_chip)\n",
        "                self.lbl_chips.append(lbl_chip)\n",
        "\n",
        "            print('--------------{} patches cropped--------------'\\\n",
        "                  .format(len(self.img_chips)))\n",
        "\n",
        "        # This part handles prediction dataset\n",
        "        else:\n",
        "            assert self.csv_name is not None\n",
        "\n",
        "            ##### Add your code to read the \"csv\" file. (Expected 1 line)\n",
        "            catalog = pd.read_csv(os.path.join(self.src_dir, self.csv_name))\n",
        "\n",
        "            ##### use \"iloc\" and \"catalog_index\" to grab one line of catalog.\n",
        "\n",
        "            self.catalog = catalog.iloc[catalog_index]\n",
        "\n",
        "            self.tile = (self.catalog[\"wrs_path\"], self.catalog[\"wrs_row\"])\n",
        "\n",
        "            img_path_ls = [self.catalog[\"img_dir\"]]\n",
        "            mask_path_ls = [self.catalog[\"mask_dir\"]]\n",
        "\n",
        "            self.meta = get_meta_from_bounds(Path(src_dir) / img_path_ls[0])\n",
        "\n",
        "            half_size = self.patch_size // 2\n",
        "\n",
        "            self.img_chips = []\n",
        "            self.coor = []\n",
        "\n",
        "            for img_path, mask_path in zip(img_path_ls, mask_path_ls):\n",
        "\n",
        "                ###### Add your code to load the image and assign it to a\n",
        "                ###### variable called \"img\".\n",
        "                ###### Use the \"load_data\" function, provided in the utility\n",
        "\n",
        "                img = load_data(os.path.join(self.src_dir, img_path),\n",
        "                                is_label = False,\n",
        "                                apply_normalization = self.apply_normalization)\n",
        "\n",
        "                img = np.transpose(img, (1, 2, 0))\n",
        "\n",
        "                ##### Load your mask again using \"load_data\" function.\n",
        "\n",
        "                mask = load_data(os.path.join(self.src_dir, mask_path),\n",
        "                                 is_label=True)\n",
        "\n",
        "                crop_ref = mask\n",
        "\n",
        "                index = patch_center_index(crop_ref, self.patch_size,\n",
        "                                           self.overlap, self.usage)\n",
        "\n",
        "                for i in range(len(index)):\n",
        "                    x = index[i][0]\n",
        "                    y = index[i][1]\n",
        "\n",
        "                    self.img_chips.append(img[x - half_size: x + half_size,\n",
        "                                              y - half_size: y + half_size, :])\n",
        "                    self.coor.append([x, y])\n",
        "\n",
        "\n",
        "\n",
        "            print('--------------{} patches cropped--------------'\\\n",
        "                  .format(len(self.img_chips)))\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        if self.usage in [\"train\", \"validation\"]:\n",
        "            image_chip = self.img_chips[index]\n",
        "            label_chip = self.lbl_chips[index]\n",
        "\n",
        "            if self.usage == \"train\" and self.transform:\n",
        "                trans_flip_ls = [m for m in self.transform if \"flip\" in m]\n",
        "                if random.randint(0, 1) and len(trans_flip_ls) > 1:\n",
        "                    trans_flip = random.sample(trans_flip_ls, 1)[0]\n",
        "                    image_chip, label_chip = flip_image_and_label(\n",
        "                        image_chip, label_chip, trans_flip\n",
        "                    )\n",
        "\n",
        "                if random.randint(0, 1) and \"rotate\" in self.transform:\n",
        "                    img_chip, lbl_chip = rotate_image_and_label(\n",
        "                        image_chip, label_chip, angle=[0,90]\n",
        "                    )\n",
        "\n",
        "            # Convert numpy arrays to torch tensors.\n",
        "            # Image chips should be: CHW if not transpose to correct order of\n",
        "            # dimensions.\n",
        "            image_tensor = torch.from_numpy(image_chip.transpose((2, 0, 1)))\\\n",
        "                .float()\n",
        "            label_tensor = torch.from_numpy(np.ascontiguousarray(label_chip))\\\n",
        "                .long()\n",
        "\n",
        "            return image_tensor, label_tensor\n",
        "        else:\n",
        "            coor = self.coor[index]\n",
        "            img_chip = self.img_chips[index]\n",
        "            image_tensor = torch.from_numpy(img_chip.transpose((2, 0, 1)))\\\n",
        "                .float()\n",
        "\n",
        "            return image_tensor, coor\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_chips)"
      ],
      "metadata": {
        "id": "T3JVuBcVqYsm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Building\n",
        "Deeplab3+ based on [Chen et al., 2024](https://link.springer.com/content/pdf/10.1007/s40747-023-01304-z.pdf)\n"
      ],
      "metadata": {
        "id": "93Q9eXHW94ng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Convolutional Neural Blocks"
      ],
      "metadata": {
        "id": "A-2tR34Nnrh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv3x3_bn_relu(nn.Module):\n",
        "    def __init__(self, inch, outch, padding = 0, stride =1, dilation = 1, groups = 1, relu = True):\n",
        "        super(Conv3x3_bn_relu, self).__init__()\n",
        "        self.applyRelu = relu\n",
        "\n",
        "        self.conv = nn.Sequential(nn.Conv2d(inch, outch, 3, \\\n",
        "                                            padding = padding, stride = stride,\\\n",
        "                                            dilation = dilation,\n",
        "                                            groups = groups),\n",
        "                                  nn.BatchNorm2d(outch))\n",
        "        if self.applyRelu:\n",
        "            self.relu = nn.ReLU(True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        if self.applyRelu:\n",
        "            out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class Conv1x1_bn_relu(nn.Module):\n",
        "    def __init__(self, inch, outch, stride = 1, padding = 0, dilation = 1,\\\n",
        "                 groups = 1, relu = True):\n",
        "        super(Conv1x1_bn_relu, self).__init__()\n",
        "        self.applyRelu = relu\n",
        "        self.conv = nn.Sequential(nn.Conv2d(inch, outch, 1, stride = stride,\\\n",
        "                                            padding = padding, \\\n",
        "                                            dilation = dilation,\n",
        "                                            groups = groups),\n",
        "                                  nn.BatchNorm2d(outch))\n",
        "\n",
        "        if self.applyRelu:\n",
        "            self.relu = nn.ReLU(True)\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x.clone())\n",
        "        if self.applyRelu:\n",
        "            x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Consecutive 2 convolution with batch normalization and ReLU activation\n",
        "class doubleConv(nn.Module):\n",
        "    def __init__(self, inch, outch):\n",
        "        super(doubleConv, self).__init__()\n",
        "        self.conv1 = Conv3x3_bn_relu(inch, outch, padding = 1)\n",
        "        self.conv2 = Conv3x3_bn_relu(outch, outch, padding = 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# skip module\n",
        "# basic unit of resnet\n",
        "class basicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, inch, outch, dilation = 1, firstStride = 1, *kwargs):\n",
        "        super(basicBlock, self).__init__()\n",
        "        self.firstBlock = (inch != outch)\n",
        "        transch = outch\n",
        "\n",
        "        if self.firstBlock:\n",
        "            self.conv0 = Conv1x1_bn_relu(inch, outch, stride = firstStride, \\\n",
        "                                         relu = False)\n",
        "\n",
        "\n",
        "        # 1st 3x3 Conv\n",
        "        self.conv1 = Conv3x3_bn_relu(inch, transch, stride =  firstStride,\\\n",
        "                                     padding = dilation,\n",
        "                                     dilation = dilation)\n",
        "        # 2nd 3x3 Conv\n",
        "        self.conv2 = Conv3x3_bn_relu(transch, outch, padding = dilation,\\\n",
        "                                     dilation = dilation,\n",
        "                                        relu = False)\n",
        "        self.relu = nn.ReLU(True)\n",
        "\n",
        "    def forward(self,x):\n",
        "        res = self.conv1(x)\n",
        "        res = self.conv2(res)\n",
        "        if self.firstBlock:\n",
        "            x = self.conv0(x)\n",
        "\n",
        "        out = self.relu(res + x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class bottleNeck(nn.Module):\n",
        "    expansion = 4\n",
        "    def __init__(self, inch, outch, dilation = 1, firstStride = 1, groups = 1,\\\n",
        "                 base_width = 64):\n",
        "        super(bottleNeck, self).__init__()\n",
        "\n",
        "        self.firstBlock = (inch != outch)\n",
        "        transch = int(outch / (self.expansion * groups * base_width / 64))\n",
        "\n",
        "        # downsample in first 1x1 convolution\n",
        "        if self.firstBlock:\n",
        "            self.conv0 = Conv1x1_bn_relu(inch, outch, stride=firstStride,\\\n",
        "                                         relu=False)\n",
        "\n",
        "        # 1x1 conv\n",
        "        self.conv1 = Conv1x1_bn_relu(inch, transch, stride=firstStride)\n",
        "        # 3x3 conv\n",
        "        self.conv2 = Conv3x3_bn_relu(transch, transch, padding = dilation, \\\n",
        "                                     dilation = dilation, groups = groups)\n",
        "        # 1x1 conv\n",
        "        self.conv3 = Conv1x1_bn_relu(transch, outch, relu=False)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = self.conv1(x)\n",
        "        res = self.conv2(res)\n",
        "        res = self.conv3(res)\n",
        "\n",
        "        if self.firstBlock:\n",
        "            x = self.conv0(x)\n",
        "\n",
        "        out = self.relu(res + x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"This module creates a user-defined number of conv+BN+ReLU layers.\n",
        "    Args:\n",
        "        in_channels (int): number of input features.\n",
        "        out_channels (int): number of output features.\n",
        "        num_conv_layers (int): Number of conv+BN+ReLU layers in the block.\n",
        "        drop_rate (float): dropout rate at the end of the block.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1,\n",
        "                 padding=1, dilation=1, num_conv_layers=2, drop_rate=0):\n",
        "        super(ConvBlock, self).__init__()\n",
        "\n",
        "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n",
        "                            stride=stride, padding=padding, dilation=dilation,\\\n",
        "                            bias=False),\n",
        "                  nn.BatchNorm2d(out_channels),\n",
        "                  nn.ReLU(inplace=True), ]\n",
        "\n",
        "        # This part has a dynamic size regarding the number\n",
        "        # of conv layers in the block.\n",
        "        layers += [nn.Conv2d(out_channels, out_channels, \\\n",
        "                             kernel_size=kernel_size,\n",
        "                             stride=stride, padding=padding, \\\n",
        "                             dilation=dilation, bias=False),\n",
        "                   nn.BatchNorm2d(out_channels),\n",
        "                   nn.ReLU(inplace=True), ] * (num_conv_layers - 1)\n",
        "\n",
        "        if drop_rate > 0 and num_conv_layers > 1:\n",
        "            layers += [nn.Dropout(drop_rate)]\n",
        "\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.block(inputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class SeLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, reduction):\n",
        "        super(SeLayer, self).__init__()\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(in_channels, in_channels // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_channels // reduction, in_channels, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "\n",
        "class ErrCorrBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, reduction=16):\n",
        "        super(ErrCorrBlock, self).__init__()\n",
        "\n",
        "        self.conv0 = nn.Sequential(nn.Conv2d(in_channels, \\\n",
        "                                             out_channels, kernel_size=1, \\\n",
        "                                             padding=0, bias=False),\n",
        "                                   nn.BatchNorm2d(out_channels))\n",
        "\n",
        "        middle_ch = in_channels // reduction\n",
        "\n",
        "        self.triple_conv = nn.Sequential(\n",
        "\n",
        "            nn.Conv2d(in_channels, middle_ch, kernel_size=1, padding=0, \\\n",
        "                      stride=1, bias=False),\n",
        "            nn.BatchNorm2d(middle_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(middle_ch, middle_ch, kernel_size=3, padding=1, \\\n",
        "                      stride=1, bias=False),\n",
        "            nn.BatchNorm2d(middle_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(middle_ch, out_channels, kernel_size=1, padding=0,\\\n",
        "                      stride=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        )\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.se = SeLayer(out_channels, reduction)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.conv0(x)\n",
        "\n",
        "        out = self.triple_conv(x)\n",
        "        #out = self.se(out)\n",
        "\n",
        "        out = self.relu(out + residual)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "TL3FDlSFnq4u"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Backbone\n",
        "- **Supported backbones:**\n",
        "- `densenet` family:\n",
        "                densenet121, densenet161, densenet169, densenet201\n",
        "- `efficientnet` family:\n",
        "                efficientnet_b0, efficientnet_b1, efficientnet_b2,\n",
        "                efficientnet_b3, efficientnet_b4, efficientnet_b5,\n",
        "                efficientnet_b6, efficientnet_b7,\n",
        "                efficientnet_v2_l, efficientnet_v2_m, efficientnet_v2_s,\n",
        "- `resnext` family:\n",
        "                resnext101_32x8d, resnext101_64x4d, `resnext50_32x4d`"
      ],
      "metadata": {
        "id": "VazUvMcvl4e5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load pre-train models for backbone"
      ],
      "metadata": {
        "id": "yqh1pkM2xh0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load pre-trained models from ResNext Family\n",
        "# # resnext50_32x4d\n",
        "# resnext50_32x4d = pretrainmodels.resnext50_32x4d(pretrained=True)\n",
        "# # resnext101_32x8d\n",
        "# resnext101_32x8d = pretrainmodels.resnext101_32x8d(pretrained=True)\n",
        "# # resnext101_64x4d\n",
        "# resnext101_64x4d = pretrainmodels.resnext101_64x4d(pretrained=True)\n",
        "# #-------------------------------------------------------------------------------\n",
        "\n",
        "# # Load pre-trained models from efficientnet Family\n",
        "# # efficientnet_b0\n",
        "# efficientnet_b0 = pretrainmodels.efficientnet_b0(pretrained=True)\n",
        "# # efficientnet_b1\n",
        "# efficientnet_b1 = pretrainmodels.efficientnet_b1(pretrained=True)\n",
        "# # efficientnet_b2\n",
        "# efficientnet_b2 = pretrainmodels.efficientnet_b2(pretrained=True)\n",
        "# # efficientnet_b3\n",
        "# efficientnet_b3 = pretrainmodels.efficientnet_b3(pretrained=True)\n",
        "# # efficientnet_b4\n",
        "# efficientnet_b4 = pretrainmodels.efficientnet_b4(pretrained=True)\n",
        "# # efficientnet_b5\n",
        "# efficientnet_b5 = pretrainmodels.efficientnet_b5(pretrained=True)\n",
        "# # efficientnet_b6\n",
        "# efficientnet_b6 = pretrainmodels.efficientnet_b6(pretrained=True)\n",
        "# # efficientnet_b7\n",
        "# efficientnet_b7 = pretrainmodels.efficientnet_b7(pretrained=True)\n",
        "# # efficientnet_v2_m\n",
        "# efficientnet_v2_m = pretrainmodels.efficientnet_v2_m(pretrained=True)\n",
        "# # efficientnet_v2_s\n",
        "# efficientnet_v2_s = pretrainmodels.efficientnet_v2_s(pretrained=True)\n",
        "# #-----------------------------------------------------------------------------\n",
        "\n",
        "# # Load pre-train models from densenet family\n",
        "# # densenet121\n",
        "# densenet121 = pretrainmodels.densenet121(pretrained=True)\n",
        "# # densenet161\n",
        "# densenet161 = pretrainmodels.densenet161(pretrained=True)\n",
        "# # densenet169\n",
        "# densenet169 = pretrainmodels.densenet169(pretrained=True)\n",
        "# # densenet201\n",
        "# densenet201 = pretrainmodels.densenet201(pretrained=True)\n",
        "# #-----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# # Load pre-trained MobileNetv2 model\n",
        "mobilenet_v2 = pretrainmodels.mobilenet_v2(pretrained=True)\n",
        "\n",
        "# # Move the model to GPU if available\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# mobilenet_v2.to(device)\n",
        "# resnext50_32x4d.to(device)\n",
        "# resnext101_32x8d.to(device)\n",
        "# resnext101_64x4d.to(device)\n",
        "\n",
        "# densenet121.to(device)\n",
        "# densenet161.to(device)\n",
        "# densenet169.to(device)\n",
        "# densenet201.to(device)\n",
        "\n",
        "# efficientnet_b0.to(device)\n",
        "# efficientnet_b1.to(device)\n",
        "# efficientnet_b2.to(device)\n",
        "# efficientnet_b3.to(device)\n",
        "# efficientnet_b4.to(device)\n",
        "# efficientnet_b5.to(device)\n",
        "# efficientnet_b6.to(device)\n",
        "# efficientnet_b7.to(device)\n",
        "# efficientnet_v2_m.to(device)\n",
        "# efficientnet_v2_s.to(device)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRq_mx81vPP6",
        "outputId": "20edf6ba-1f71-4fa5-fdd8-19c7eccd9223"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n",
            "100%|██████████| 13.6M/13.6M [00:00<00:00, 36.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backbone Class"
      ],
      "metadata": {
        "id": "hmx3NfuG7HyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Backbone(nn.Module):\n",
        "    def __init__(self, backbone_name, num_classes, num_input_channels,\n",
        "                 weights=None, weight_handler=\"copy\"):\n",
        "        \"\"\"\n",
        "        A PyTorch module for creating a custom model using an optionally\n",
        "        pre-trained torchvision model as a backbone. The model will have\n",
        "        modified input and output layers, depending on the parameters given.\n",
        "\n",
        "        Args:\n",
        "            backbone_name : str\n",
        "                The name of the pre-trained torchvision model to use as a\n",
        "                backbone.\n",
        "            num_classes : int\n",
        "                The number of output classes.\n",
        "            num_input_channels :int\n",
        "                The number of input channels.\n",
        "            weights : str, optional\n",
        "                The weights to initialize the backbone with. If not given, the\n",
        "                backbone will be initialized randomly.\n",
        "            weight_handler : str, optional\n",
        "                Specifies how to handle the weights for extra input channels if\n",
        "                num_input_channels > 3. Available options are: 'copy': copy\n",
        "                weights from the already initialized channels. 'random':\n",
        "                initialize weights randomly. Defaults to 'copy'.\n",
        "        Supported backbones:\n",
        "            densenet family:\n",
        "                densenet121, densenet161, densenet169, densenet201\n",
        "            efficientnet family:\n",
        "                efficientnet_b0, efficientnet_b1, efficientnet_b2,\n",
        "                efficientnet_b3, efficientnet_b4, efficientnet_b5,\n",
        "                efficientnet_b6, efficientnet_b7,\n",
        "                efficientnet_v2_l, efficientnet_v2_m, efficientnet_v2_s,\n",
        "            resnext family:\n",
        "                resnext101_32x8d, resnext101_64x4d, resnext50_32x4d\n",
        "        \"\"\"\n",
        "        super(Backbone, self).__init__()\n",
        "\n",
        "        assert weight_handler in [\"copy\", \"random\"], \\\n",
        "            \"Unrecognized 'weight_handler'.\"\n",
        "\n",
        "        # Functions to get the first and last layer names based on model type\n",
        "        def get_first_layer(backbone_name):\n",
        "            if 'efficientnet' in backbone_name:\n",
        "                return 'features[0][0]'\n",
        "            elif 'densenet' in backbone_name:\n",
        "                return 'features[0]'\n",
        "            elif 'resnext' in backbone_name:\n",
        "                return 'conv1'\n",
        "            else:\n",
        "                raise ValueError('Unrecognized backbone architecture.')\n",
        "\n",
        "        def get_last_layer(backbone_name):\n",
        "            if 'efficientnet' in backbone_name:\n",
        "                return 'classifier[1]'\n",
        "            elif 'densenet' in backbone_name:\n",
        "                return 'classifier'\n",
        "            elif 'resnext' in backbone_name:\n",
        "                return 'fc'\n",
        "            else:\n",
        "                raise ValueError('Unrecognized model type')\n",
        "\n",
        "        # Load the backbone model\n",
        "        self.backbone = getattr(pretrainmodels, backbone_name)(weights=weights)\n",
        "\n",
        "        # Modify the first convolution layer\n",
        "        original_conv = eval('self.backbone.' + get_first_layer(backbone_name))\n",
        "        new_conv = nn.Conv2d(num_input_channels, original_conv.out_channels,\n",
        "                             kernel_size=original_conv.kernel_size,\n",
        "                             stride=original_conv.stride,\n",
        "                             padding=original_conv.padding,\n",
        "                             bias=original_conv.bias)\n",
        "\n",
        "        # weight handling\n",
        "        if weights is not None:\n",
        "            new_conv.weight.data[:,:3,:,:] = original_conv.weight.data\n",
        "            if num_input_channels > 3:\n",
        "                if weight_handler == \"copy\":\n",
        "                    new_conv.weight.data[:,3:,:,:] = original_conv.weight.data[\n",
        "                        :,:num_input_channels-3,:,:\n",
        "                    ]\n",
        "                else:\n",
        "                    nn.init.kaiming_normal_(new_conv.weight.data[:,3:,:,:])\n",
        "\n",
        "        # Replace the first convolution layer\n",
        "        exec('self.backbone.' + get_first_layer(backbone_name) + '= new_conv')\n",
        "\n",
        "        # Modify the classifier layer\n",
        "        original_fc = eval('self.backbone.' + get_last_layer(backbone_name))\n",
        "        new_fc = nn.Linear(original_fc.in_features, num_classes)\n",
        "\n",
        "        # Replace the classifier layer\n",
        "        exec('self.backbone.' + get_last_layer(backbone_name) + '= new_fc')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Si-SyeHYilAw"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ASPP Structure"
      ],
      "metadata": {
        "id": "lHersbFS7m9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ASPP structure\n",
        "class ASPP(nn.Module):\n",
        "    def __init__(self, inch, rates, stagech):\n",
        "        super(ASPP, self).__init__()\n",
        "        '''\n",
        "        This class generates the ASPP module introduced in\n",
        "        DeepLabv3: https://arxiv.org/pdf/1706.05587.pdf, which\n",
        "         concatenates 4 parallel atrous spatial pyramid pooling and the\n",
        "         image level features. For more detailed\n",
        "         information, please refer to the paper of DeepLabv3\n",
        "\n",
        "         Args:\n",
        "            inch -- (int) Depth of the input tensor\n",
        "            rates -- (list) A list of rates of the parallel atrous convolution,\n",
        "                      including that for the 1x1 convolution\n",
        "            stagech -- (int) Depth of output tensor for each of the parallel\n",
        "                        atrous convolution\n",
        "\n",
        "         Returns:\n",
        "            A tensor after a 1x1 convolution of the concatenated ASPP features\n",
        "        '''\n",
        "\n",
        "        # create stages\n",
        "        self.rates = rates\n",
        "        self.inch = inch\n",
        "        self.stagech = stagech\n",
        "\n",
        "        self.stages = self.makeStages()\n",
        "        # global feature\n",
        "        self.globe = nn.Sequential(nn.AdaptiveAvgPool2d((1,1)), \\\n",
        "                                   Conv1x1_bn_relu(inch, stagech, relu = False))\n",
        "        # self.conv1x1 = Conv1x1_bn_relu(inch, stagech, relu = False)\n",
        "        # self.conv = Conv3x3_bn_relu(inch * 2, inch, padding = 1)\n",
        "        self.conv = Conv1x1_bn_relu(stagech*(len(rates) + 1), stagech,\\\n",
        "                                    relu = False)\n",
        "\n",
        "    def makeStages(self):\n",
        "        outch = self.stagech\n",
        "        inch = self.inch\n",
        "        stages = []\n",
        "        for rate in self.rates:\n",
        "            if rate == 1:\n",
        "                stage = Conv1x1_bn_relu(inch, outch, relu = False)\n",
        "\n",
        "            else:\n",
        "                stage = Conv3x3_bn_relu(inch, outch, padding =rate, \\\n",
        "                                        dilation = rate, relu = False)\n",
        "\n",
        "            stages.append(stage)\n",
        "        return nn.ModuleList(stages)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_size = x.size()\n",
        "        # x1 = [F.interpolate(stage(x), size=x_size[-2:], mode=\"bilinear\",\\\n",
        "        #  align_corners=True) for stage in self.stages]\n",
        "        x0 = [stage(x) for stage in self.stages]\n",
        "\n",
        "        # global feature\n",
        "        x1 = self.globe(x)\n",
        "        x1 = F.interpolate(x1, size = x_size[-2:], mode = \"bilinear\", \\\n",
        "                           align_corners = True)\n",
        "\n",
        "        x = torch.cat(x0 + [x1], 1)\n",
        "        x = self.conv(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "qclFDk_07mQp"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model (DeepLabv3+)"
      ],
      "metadata": {
        "id": "XVMJnNbHr5io"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IaQO_HOzr53n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ASPPInchByBackbone = {\n",
        "    \"resnet\": 2048,\n",
        "    \"Xception\": 2048\n",
        "}\n",
        "\n",
        "quaterOutchByBackbone = {\n",
        "    \"resnet\": 256,\n",
        "    \"Xception\": 128\n",
        "}\n",
        "\n",
        "class deeplabv3plus2(nn.Module):\n",
        "    def __init__(self, inch, classNum, backbone = mobilenet_v2, \\\n",
        "                 outStride = 16, rates = [1, 6, 12, 18]):\n",
        "        super(deeplabv3plus2, self).__init__()\n",
        "\n",
        "        # backbone\n",
        "        self.backbone = backbone(inch, outStride = outStride)\n",
        "\n",
        "        # ASPP\n",
        "        ASPPinch = ASPPInchByBackbone[backbone.__name__.rstrip('0123456789')]\n",
        "        ASPPoutch = ASPPinch // 8\n",
        "        self.ASPP = ASPP(ASPPinch, rates=rates, stagech=ASPPoutch)\n",
        "\n",
        "        # decoder\n",
        "        quaterOutch = quaterOutchByBackbone[backbone.__name__.rstrip('0123456789')]\n",
        "        self.conv0 = Conv1x1_bn_relu(quaterOutch, ASPPoutch) # 1/4 of origin\n",
        "        self.up1 = nn.ConvTranspose2d(256, 256, 6, stride=4, padding=1)\n",
        "        self.last_conv = nn.Sequential(Conv3x3_bn_relu(ASPPoutch*2, 256, 1),\n",
        "                                       Conv3x3_bn_relu(256, 256, 1),\n",
        "                                       nn.Conv2d(256, classNum, 1))\n",
        "        self.up2 = nn.ConvTranspose2d(classNum, classNum, 6, stride=4, padding=1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x0, x = self.backbone(x)\n",
        "        x = self.ASPP(x)\n",
        "\n",
        "        # decoder\n",
        "        x0 = self.conv0(x0)\n",
        "        x = self.up1(x)\n",
        "        x = torch.cat([x, x0], 1)\n",
        "        x = self.last_conv(x)\n",
        "        x = self.up2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "Iv7azLGq97cB"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Fitting (Training and Validating)"
      ],
      "metadata": {
        "id": "fVI_rMXJmhzi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "uRb3qfDwmvrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(trainData, model, optimizer, criterion, device, train_loss=[]):\n",
        "    \"\"\"\n",
        "        Train the model using provided training dataset.\n",
        "        Params:\n",
        "\n",
        "                custom dataset (AquacultureData).\n",
        "            model -- Choice of segmentation model.\n",
        "            optimizer -- Chosen optimization algorithm to update model\n",
        "                parameters.\n",
        "            criterion -- Chosen function to calculate loss over training\n",
        "                samples.\n",
        "            gpu (bool, optional) -- Decide whether to use GPU, default is True.\n",
        "            train_loss (empty list, optional) -- ???????????????????????????\n",
        "    \"\"\"\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Mini batch iteration\n",
        "    train_epoch_loss = 0\n",
        "    train_batches = len(trainData)\n",
        "\n",
        "    for img_chips, labels in trainData:\n",
        "\n",
        "        img = img_chips.to(device)\n",
        "        label = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred = model(img)\n",
        "\n",
        "        loss = eval(criterion)(pred, label)\n",
        "        train_epoch_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss.append(train_epoch_loss / train_batches)\n",
        "    print('Training loss: {:.4f}'.format(train_epoch_loss / train_batches))"
      ],
      "metadata": {
        "id": "JxTYXdwHiRDH"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Validation"
      ],
      "metadata": {
        "id": "KSPpuKV9m27O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(valData, model, criterion, device, val_loss=[]):\n",
        "    \"\"\"\n",
        "        Evaluate the model on separate Landsat scenes.\n",
        "        Params:\n",
        "            valData (DataLoader object) -- Batches of image chips from PyTorch\n",
        "                custom dataset(AquacultureData)\n",
        "            model -- Choice of segmentation Model.\n",
        "            criterion -- Chosen function to calculate loss over validation\n",
        "                samples.\n",
        "            buffer: Buffer added to the targeted grid when creating dataset.\n",
        "                This allows loss to calculate at non-buffered region.\n",
        "            gpu (binary,optional): Decide whether to use GPU, default is True\n",
        "            valLoss (empty list): To record average loss for each epoch\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # mini batch iteration\n",
        "    eval_epoch_loss = 0\n",
        "\n",
        "    for img_chips, labels in valData:\n",
        "\n",
        "        img = Variable(img_chips, requires_grad=False)\n",
        "        label = Variable(labels, requires_grad=False)\n",
        "\n",
        "        img = img_chips.to(device)\n",
        "        label = labels.to(device)\n",
        "\n",
        "        pred = model(img)\n",
        "\n",
        "        loss = eval(criterion)(pred, label)\n",
        "        eval_epoch_loss += loss.item()\n",
        "\n",
        "    print('validation loss: {}'.format(eval_epoch_loss / len(valData)))\n",
        "\n",
        "    if val_loss != None:\n",
        "        val_loss.append(float(eval_epoch_loss / len(valData)))"
      ],
      "metadata": {
        "id": "OpYPJxVlm7fS"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Epoch Iterator for model training and validation process"
      ],
      "metadata": {
        "id": "K2aGkVBwnBZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def epochIterater(trainData, valData, model, criterion, WorkingFolder,\n",
        "                  initial_lr, num_epochs):\n",
        "    \"\"\"\n",
        "    Epoch iteration for train and evaluation.\n",
        "\n",
        "    Arguments:\n",
        "    trainData (dataloader object): Batch grouped data to train the model.\n",
        "    evalData (dataloader object): Batch grouped data to evaluate the model.\n",
        "    model (pytorch.nn.module object): initialized model.\n",
        "    initial_lr(float): The initial learning rate.\n",
        "    num_epochs (int): User-defined number of epochs to run the model.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    train_loss = []\n",
        "    val_loss = []\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    if device.type == \"cuda\":\n",
        "        print('----------GPU available----------')\n",
        "        model = model.to(device)\n",
        "    else:\n",
        "        print('----------No GPU available, using CPU instead----------')\n",
        "        model = model\n",
        "\n",
        "    writer = SummaryWriter(WorkingFolder)\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                           lr=initial_lr,\n",
        "                           betas=(0.9, 0.999),\n",
        "                           eps=1e-08,\n",
        "                           weight_decay=5e-4,\n",
        "                           amsgrad=False)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
        "                                          step_size=3,\n",
        "                                          gamma=0.90)\n",
        "\n",
        "    for t in range(num_epochs):\n",
        "        print(\"Epoch [{}/{}]\".format(t + 1, num_epochs))\n",
        "        start_epoch = datetime.now()\n",
        "\n",
        "        train(trainData, model, optimizer, criterion, device,\n",
        "              train_loss=train_loss)\n",
        "        validate(valData, model, criterion, device, val_loss=val_loss)\n",
        "\n",
        "        scheduler.step()\n",
        "        print(\"LR: {}\".format(scheduler.get_last_lr()))\n",
        "\n",
        "        writer.add_scalars(\"Loss\",\n",
        "                           {\"train loss\": train_loss[t],\n",
        "                            \"validation loss\": val_loss[t]},\n",
        "                           t + 1)\n",
        "\n",
        "    writer.close()\n",
        "\n",
        "    duration_in_sec = (datetime.now() - start_epoch).seconds\n",
        "    duration_format = str(timedelta(seconds=duration_in_sec))\n",
        "    print(\"--------------- Training finished in {} ---------------\"\\\n",
        "          .format(duration_format))"
      ],
      "metadata": {
        "id": "JMWU7Qf-nI93"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation and Accuracy Metrics\n"
      ],
      "metadata": {
        "id": "1fD9Z09onMiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Evaluator(object):\n",
        "    def __init__(self, num_class):\n",
        "        self.num_class = num_class\n",
        "        self.confusion_matrix = np.zeros((self.num_class,)*2)\n",
        "\n",
        "    def Pixel_Accuracy(self):\n",
        "        Acc = np.diag(self.confusion_matrix).sum() / self.confusion_matrix.sum()\n",
        "        return Acc\n",
        "\n",
        "    def Pixel_Accuracy_Class(self):\n",
        "        Acc = np.diag(self.confusion_matrix) / self.confusion_matrix.sum(axis=1)\n",
        "        Acc = np.nanmean(Acc)\n",
        "        return Acc\n",
        "\n",
        "    def Mean_Intersection_over_Union(self):\n",
        "        MIoU = np.diag(self.confusion_matrix) / (\n",
        "                    np.sum(self.confusion_matrix, axis=1) +\n",
        "                    np.sum(self.confusion_matrix, axis=0) -\n",
        "                    np.diag(self.confusion_matrix))\n",
        "        MIoU = np.nanmean(MIoU)\n",
        "        return MIoU\n",
        "\n",
        "    def Frequency_Weighted_Intersection_over_Union(self):\n",
        "        freq = np.sum(self.confusion_matrix, axis=1) /\\\n",
        "            np.sum(self.confusion_matrix)\n",
        "        iu = np.diag(self.confusion_matrix) / (\n",
        "                    np.sum(self.confusion_matrix, axis=1) +\n",
        "                    np.sum(self.confusion_matrix, axis=0) -\n",
        "                    np.diag(self.confusion_matrix)\n",
        "                )\n",
        "\n",
        "        FWIoU = (freq[freq > 0] * iu[freq > 0]).sum()\n",
        "        return FWIoU\n",
        "\n",
        "    def _generate_matrix(self, gt_image, pre_image):\n",
        "        mask = (gt_image >= 0) & (gt_image < self.num_class)\n",
        "        label = self.num_class * gt_image[mask].astype('int') + pre_image[mask]\n",
        "        count = np.bincount(label, minlength=self.num_class**2)\n",
        "        confusion_matrix = count.reshape(self.num_class, self.num_class)\n",
        "        return confusion_matrix\n",
        "\n",
        "    def add_batch(self, gt_image, pre_image):\n",
        "        assert gt_image.shape == pre_image.shape\n",
        "        self.confusion_matrix += self._generate_matrix(gt_image, pre_image)\n",
        "\n",
        "    def reset(self):\n",
        "        self.confusion_matrix = np.zeros((self.num_class,) * 2)\n",
        "\n",
        "##==============================================================================\n",
        "\n",
        "def do_accuracy_evaluation(model, dataloader, num_classes):\n",
        "    evaluator = Evaluator(num_classes)\n",
        "\n",
        "    model.eval()\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            images, labels = data\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "            # add batch to evaluator\n",
        "            evaluator.add_batch(labels.cpu().numpy(), preds.cpu().numpy())\n",
        "\n",
        "    # calculate evaluation metrics\n",
        "    pixel_accuracy = evaluator.Pixel_Accuracy()\n",
        "    mean_accuracy = evaluator.Pixel_Accuracy_Class()\n",
        "    mean_IoU = evaluator.Mean_Intersection_over_Union()\n",
        "    frequency_weighted_IoU = evaluator\\\n",
        "        .Frequency_Weighted_Intersection_over_Union()\n",
        "\n",
        "    return pixel_accuracy, mean_accuracy, mean_IoU, frequency_weighted_IoU"
      ],
      "metadata": {
        "id": "iPlxN1oDnTBw"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running Model through the pipeline"
      ],
      "metadata": {
        "id": "R0s31dcP8eku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining model parameters"
      ],
      "metadata": {
        "id": "FEfjpCy4FNtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_dir = \"/content/gdrive/MyDrive/adleo/assignment5/A5_resources\"\n",
        "dataset_name = \"Global\"\n",
        "transform = [\"hflip\", \"vflip\", \"rotate\"]\n",
        "\n",
        "n_classes = 2\n",
        "in_channels = 6\n",
        "filter_config = (32, 64, 128, 256, 512, 1024)\n",
        "dropout_rate = 0.1\n",
        "\n",
        "criterion = \"BalancedTverskyFocalCELoss()\"\n",
        "WorkingFolder = \"/content/gdrive/MyDrive/adleo/adleo_project_test\"\n",
        "initial_lr = 0.15\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "5l8KQaGu8jD8"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a `train_dataset` object\n",
        "Using `datasetloader` class to create the train_dataset object.\n",
        "\n"
      ],
      "metadata": {
        "id": "4QeQAhZeFRgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasetloader(src_dir,\n",
        "                                usage = \"train\",\n",
        "                                dataset_name = dataset_name,\n",
        "                                apply_normalization = True,\n",
        "                                transform = transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "gA6_cEFP9DaM",
        "outputId": "320e0dc9-6e78-4d14-e34a-686741afb9c4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1188 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "load_data() got an unexpected keyword argument 'apply_normalization'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-c576b1709e29>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_dataset = datasetloader(src_dir,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                 \u001b[0musage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                 \u001b[0mdataset_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                 \u001b[0mapply_normalization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                 transform = transform)\n",
            "\u001b[0;32m<ipython-input-24-7154320e0f00>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src_dir, usage, dataset_name, apply_normalization, transform, csv_name, patch_size, overlap, catalog_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m             for img_path, lbl_path in tqdm.tqdm(zip(img_fnames, lbl_fnames),\n\u001b[1;32m     43\u001b[0m                                                 total=len(img_fnames)):\n\u001b[0;32m---> 44\u001b[0;31m                 img_chip = load_data(\n\u001b[0m\u001b[1;32m     45\u001b[0m                     \u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                     \u001b[0mapply_normalization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: load_data() got an unexpected keyword argument 'apply_normalization'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a `PyTorch` data loader called `train_loader` that loads data from the `train_dataset`, splits it into batches, convert is to tensor and moves the data to GPU if available."
      ],
      "metadata": {
        "id": "j0fqlXccF6sI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size = 16,\n",
        "                          shuffle = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "kLVJE3btF_mh",
        "outputId": "2249431c-2738-4456-d97e-e4a2b5ef6c96"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-78a72e99ee28>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_loader = DataLoader(train_dataset,\n\u001b[0m\u001b[1;32m      2\u001b[0m                           \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                           shuffle = True)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Validation dataset\n",
        "Create a `validation_dataset` object using the `datasetloader` class."
      ],
      "metadata": {
        "id": "WSg_OGOWGQzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataset = AquacultureData(src_dir,\n",
        "                                     usage=\"validation\",\n",
        "                                     dataset_name=dataset_name,\n",
        "                                     apply_normalization=False)\n",
        "\n",
        "val_loader = DataLoader(validation_dataset,\n",
        "                        batch_size = 1,\n",
        "                        shuffle = False)"
      ],
      "metadata": {
        "id": "CtphK5DaGfcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize the model"
      ],
      "metadata": {
        "id": "60_UjYWeGtJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deeplabv3plus2_model = deeplabv3plus2_model()"
      ],
      "metadata": {
        "id": "sFnMUTckGx8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fit the `deeplabv3plus2_model` using `epochIterater`"
      ],
      "metadata": {
        "id": "BHYcOm_OG6xN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochIterater(train_loader,\n",
        "              val_loader,\n",
        "              model,\n",
        "              criterion,\n",
        "              WorkingFolder,\n",
        "              initial_lr,\n",
        "              epochs)"
      ],
      "metadata": {
        "id": "EXe5GhZNG_4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the model parameters"
      ],
      "metadata": {
        "id": "d7uuNNugHJwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(),\n",
        "           os.path.join(Path(WorkingFolder), \"trained_unet_final_state.pth\"))"
      ],
      "metadata": {
        "id": "jtTWp5VTHM7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate model performance"
      ],
      "metadata": {
        "id": "RBuyLlVIHO3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = deeplabv3plus2_model\n",
        "pixel_acc, mean_acc, mean_iou, fw_iou = do_accuracy_evaluation(model,\n",
        "                                                               val_loader,\n",
        "                                                               num_classes = 10)\n",
        "print(\"Pixel Accuracy:\", pixel_acc,\"\\n\",\n",
        "      \"Mean Accuracy:\", mean_acc, \"\\n\",\n",
        "      \"Mean IoU:\", mean_iou, \"\\n\",\n",
        "      \"Frequency Weighted IoU:\", fw_iou)"
      ],
      "metadata": {
        "id": "DZNb5maVHS9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction using DeepLabv3+2 Model"
      ],
      "metadata": {
        "id": "0WFRD47AHqOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def do_prediction(testData, model, overlap, device, save_dir):\n",
        "    \"\"\"\n",
        "    Use train model to predict on unseen data.\n",
        "    Arguments:\n",
        "            testData (custom iterator) -- Batches of image chips from PyTorch\n",
        "                                          custom dataset.\n",
        "            model (ordered Dict) -- trained model.\n",
        "            overlap (int) -- amount of overlap between prediction chips.\n",
        "            device (str) -- Either \"cpu\" or \"cuda\".\n",
        "            save_dir (str) -- Directory to save the prediction output.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create directories to save the predicted output\n",
        "    save_dir_hard = Path(save_dir) / \"HardScore\"\n",
        "    save_dir_soft = Path(save_dir) / \"SoftScore\"\n",
        "\n",
        "    os.makedirs(save_dir_hard, exist_ok=True)\n",
        "    os.makedirs(save_dir_soft, exist_ok=True)\n",
        "\n",
        "    # Start inference on test data\n",
        "    print(\"--------------------- Start Inference(Test) ---------------------\")\n",
        "    start = datetime.now()\n",
        "\n",
        "    # Get the test data, metadata, and tile information\n",
        "    # add your code here\n",
        "    testData, meta, tile = testData\n",
        "\n",
        "    # Define the output file names and metadata for the hard and soft scores\n",
        "    name_prob = \"prob_c{}_r{}\".format(tile[0], tile[1])\n",
        "    name_crisp = \"crisp_c{}_r{}.rst\".format(tile[0], tile[1])\n",
        "\n",
        "    meta_hard = meta.copy()\n",
        "    meta_hard.update({\n",
        "        \"dtype\": \"uint8\",\n",
        "        \"count\": 1,\n",
        "    })\n",
        "\n",
        "    meta_soft = meta.copy()\n",
        "    meta_soft.update({\n",
        "        \"dtype\": \"float32\",\n",
        "        \"count\": 1,\n",
        "    })\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    ##### Add your code to put the model in evaluation mode. (1 line)\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ##### Create a canvas (call it \"h_canvas\") with the same height, width and\n",
        "    ##### datatype from \"meta_hard\" to hold the score values and initialize it\n",
        "    ##### to zeros. Add your code here. (Expected 1 line)\n",
        "    h_canvas = np.zeros((1, meta_hard[\"height\"], meta_hard[\"width\"]),\n",
        "                        dtype=meta_hard[\"dtype\"])\n",
        "\n",
        "    canvas_score_ls = []\n",
        "\n",
        "\n",
        "    # Loop over batches of image chips and indices.\n",
        "    for img_chips, index_batch in testData:\n",
        "        img = Variable(img_chips, requires_grad=False)\n",
        "        img = img_chips.to(device) # size: B X in_channels X W X H\n",
        "\n",
        "        ##### Forward pass through the model to get the predictions and assign\n",
        "        ####  it to a variable called \"pred\".\n",
        "        ##### Add your code here. (Expected 1 line)\n",
        "        pred = model(img)\n",
        "\n",
        "        ##### Normalize the model output using \"softmax\" And assign it to a\n",
        "        ##### variable called \"pred_prob\".\n",
        "        ##### Add your code here (Expected 1 line)\n",
        "        pred_prob = F.softmax(pred, dim=1)\n",
        "\n",
        "        # Get the dimensions of the prediction\n",
        "        batch, n_class, height, width = pred_prob.size()\n",
        "\n",
        "        # Calculate the score width and score height based on the overlap\n",
        "        # parameter\n",
        "        score_width = (width // 2) - overlap\n",
        "        score_height = (height // 2) - overlap\n",
        "\n",
        "        # Loop over the batch and assign the predicted scores to the canvas\n",
        "        for i in range(batch):\n",
        "\n",
        "            # creating a new tuple index containing the coordinates, which makes\n",
        "            # it easier to index into the \"h_canvas\" and arrays in the\n",
        "            # \"canvas_score_ls\" later on in the code.\n",
        "            index = (index_batch[0][i], index_batch[1][i])\n",
        "\n",
        "            # Get the hard scores by taking the argmax of the prediction\n",
        "            prediction_hard = pred_prob.max(dim=1)[1][\n",
        "                :, overlap:-overlap, overlap:-overlap\n",
        "            ].cpu().numpy()[i, :, :]\n",
        "\n",
        "            # add the batch dimension to the \"prediction_hard\" array and\n",
        "            # convert its data types.\n",
        "            prediction_hard = np.expand_dims(prediction_hard, axis=0)\\\n",
        "            .astype(meta_hard[\"dtype\"])\n",
        "\n",
        "            # The \"prediction_hard\" values are assigned to a slice of\n",
        "            # \"h_canvas\", effectively updating the pixels in the original image\n",
        "            # corresponding to the current image chip in the batch with the\n",
        "            # predicted values for that chip.\n",
        "            ##### Add your code here. (Expected 1 line)\n",
        "            h_canvas[\n",
        "                :, index[0] - score_width : index[0] + score_width,\n",
        "                index[1] - score_height : index[1] + score_height\n",
        "            ] = prediction_hard\n",
        "\n",
        "\n",
        "            for n in range(1, n_class):\n",
        "                # Extract probability map for class n from predicted\n",
        "                # probabilities tensor\n",
        "                prediction_soft = pred_prob[:, n, :, :]\\\n",
        "                    .data[i][overlap:-overlap, overlap:-overlap]\\\n",
        "                    .cpu().numpy() * 100\n",
        "                # Add an extra dimension to the probability map to match the\n",
        "                # expected shape\n",
        "                prediction_soft = np.expand_dims(prediction_soft, axis=0)\\\n",
        "                    .astype(meta_soft[\"dtype\"])\n",
        "\n",
        "                try:\n",
        "                    # Update existing canvas for class n w/new probability map\n",
        "                    canvas_score_ls[n][\n",
        "                        :, index[0] - score_width : index[0] + score_width,\n",
        "                        index[1] - score_height : index[1] + score_height\n",
        "                    ] = prediction_soft\n",
        "                except:\n",
        "                    # Create a new canvas for class n and initialize it with\n",
        "                    # zeros\n",
        "                    canvas_score_single = np.zeros(\n",
        "                        (1, meta_soft['height'], meta_soft['width']),\n",
        "                        dtype=meta_soft['dtype']\n",
        "                    )\n",
        "\n",
        "                    # Update the new canvas with the new probability map slice\n",
        "                    # by slice\n",
        "                    canvas_score_single[\n",
        "                        :, index[0] - score_width: index[0] + score_width,\n",
        "                        index[1] - score_height: index[1] + score_height\n",
        "                    ] = prediction_soft\n",
        "\n",
        "                    # Add the new canvas to the list of canvases for all classes\n",
        "                    canvas_score_ls.append(canvas_score_single)\n",
        "\n",
        "    # write the hard classification results to an output raster.\n",
        "    ##### Use \"save_dir_hard\", \"name_crisp\" and \"meta_hard\".\n",
        "    ##### Add your code here. (Expected 2 line)\n",
        "    with rasterio.open(os.path.join(save_dir_hard, name_crisp)\\\n",
        "                       ,'w', **meta_hard) as rstr:\n",
        "                       rstr.write(h_canvas)\n",
        "\n",
        "\n",
        "    # loop through each class (excluding the background class) and creates a\n",
        "    # new raster file for each class.\n",
        "    ##### Add your code here. (Expected 4 line)\n",
        "    ##### hint: use this code to get a proper name for the prediction output\n",
        "    ##### for each class: name_prob_updated = f\"{name_prob}_Cat_{n}.tif\"\n",
        "\n",
        "    for n in range(1, n_class):\n",
        "        name_prob_updated = f\"{name_prob}_Cat_{n}.tif\"\n",
        "        with rasterio.open(os.path.join(save_dir_soft, name_prob_updated), \\\n",
        "                           'w', **meta_soft) as rstr:\n",
        "                           rstr.write(canvas_score_ls[n])\n",
        "\n",
        "\n",
        "\n",
        "    duration_in_sec = (datetime.now() - start).seconds\n",
        "    duration_format = str(timedelta(seconds=duration_in_sec))\n",
        "    print(\"---------------- Inference finished in {} seconds ----------------\"\\\n",
        "          .format(duration_format))"
      ],
      "metadata": {
        "id": "_M3u4D5bHpjh"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Initial Parameters for the prediction"
      ],
      "metadata": {
        "id": "EkJItWRaIAn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_dir = \"/content/gdrive/MyDrive/adleo/assignment5/A5_resources\"\n",
        "dataset_name = \"Fine_tune_dataset\"\n",
        "transform = [\"hflip\", \"vflip\", \"rotate\"]\n",
        "\n",
        "n_classes = 2\n",
        "in_channels = 6\n",
        "filter_config = (32, 64, 128, 256, 512, 1024)\n",
        "dropout_rate = 0.15\n",
        "\n",
        "criterion = \"BalancedTverskyFocalCELoss()\"\n",
        "optimizer = optim.Adam(Unet_model.parameters(), lr=initial_lr)\n",
        "WorkingFolder = \"/content/gdrive/MyDrive/adleo/assignment5\"\n",
        "initial_lr = 0.01\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "YR77524DH4Wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the train_dataset"
      ],
      "metadata": {
        "id": "_ryHVWdBIOnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasetloader(src_dir,\n",
        "                                usage=\"train\",\n",
        "                                dataset_name=dataset_name,\n",
        "                                apply_normalization=False,\n",
        "                                transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size = 16,\n",
        "                          shuffle = True)"
      ],
      "metadata": {
        "id": "8Ezv-oNOIR6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load validation_dataset"
      ],
      "metadata": {
        "id": "Rzi3A3M8IUQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataset = AquacultureData(src_dir,\n",
        "                                     usage=\"validation\",\n",
        "                                     dataset_name=dataset_name,\n",
        "                                     apply_normalization=False)\n",
        "\n",
        "val_loader = DataLoader(validation_dataset,\n",
        "                        batch_size = 1,\n",
        "                        shuffle = False)"
      ],
      "metadata": {
        "id": "DEMV8vUuIX1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_pred(usage, csv_name, patch_size, overlap, catalog_index):\n",
        "    pred_dataset = datasetloder(src_dir,\n",
        "                                   usage = usage,\n",
        "                                   apply_normalization=False,\n",
        "                                   csv_name = csv_name,\n",
        "                                   patch_size = patch_size,\n",
        "                                   overlap = overlap,\n",
        "                                   catalog_index=catalog_index)\n",
        "\n",
        "    data_loader = DataLoader(pred_dataset, batch_size=1, shuffle=False)\n",
        "    meta = pred_dataset.meta\n",
        "    tile = pred_dataset.tile\n",
        "\n",
        "    return data_loader, meta, tile"
      ],
      "metadata": {
        "id": "x9GktFRyJohx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make the prediction"
      ],
      "metadata": {
        "id": "3-SrfA2ANs_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tile_count = len(pd.read_csv(os.path.join(src_dir, csv_name)))\n",
        "for i in range(tile_count):\n",
        "  pred_data = load_data_pred(\"inference\", csv_name, patch_size, overlap, i)\n",
        "  do_prediction(pred_data, model, overlap, device, save_dir)"
      ],
      "metadata": {
        "id": "1KhH0lVrJqVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot the predicted image"
      ],
      "metadata": {
        "id": "yUceCSJvNwuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the image location\n",
        "img_src = \"/predictions/SoftScore/prob_c11_r60_Cat_1.tif\"\n",
        "WorkingFolder = WorkingFolder\n",
        "rast_img_file = (WorkingFolder + img_src)\n",
        "# Load the image\n",
        "with rasterio.open(rast_img_file) as src:\n",
        "  pred_img1 = src.read(1)\n",
        "\n",
        "  plt.imshow(pred_img1, cmap=\"tab10\")\n",
        "  plt.title(\"Predicted by Unet\")\n",
        "  plt.colorbar()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "hSzgbTE6MW0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-cZ4Vm4SNvKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "\n",
        "Chen LC et al., (2018)\n",
        "Encoder-decoder with atrous separable convolution for semantic\n",
        "image segmentation, In: Proceedings of the European conference\n",
        "on computer vision (ECCV). 801–818"
      ],
      "metadata": {
        "id": "ILQecsuySmxC"
      }
    }
  ]
}